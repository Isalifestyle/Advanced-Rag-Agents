{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGCHAIN_API_KEY: None\n",
      "LANGSMITH_API_KEY: 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\n",
      "LANGSMITH_PROJECT: pr-prickly-rush-65\n",
      "tracing: true\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os, getpass\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition \n",
    "import langsmith\n",
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Ensure environment variables are set\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please set {var}: \")\n",
    "\n",
    "\n",
    "\n",
    "# Debug: Print out the environment variables to ensure they're loaded correctly\n",
    "print(f\"LANGCHAIN_API_KEY: {os.getenv('LANGCHAIN_API_KEY')}\")\n",
    "print(f\"LANGSMITH_API_KEY: {os.getenv('LANGSMITH_API_KEY')}\")\n",
    "print(f\"LANGSMITH_PROJECT: {os.getenv('LANGSMITH_PROJECT')}\")\n",
    "print(f\"tracing: {os.getenv('LANGCHAIN_TRACING_V2')}\")\n",
    "\n",
    "\n",
    "# Initialize the LLM (make sure this is correct and that Groq model supports LangSmith tracing)\n",
    "llm = ChatGroq(api_key=\"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\", model=\"mistral-saba-24b\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from langgraph.graph import MessagesState\n",
    "from typing import Annotated\n",
    "from typing import List\n",
    "\n",
    "class State(MessagesState):\n",
    "    query:str\n",
    "    subqueries: str\n",
    "    human_feedback_full: str\n",
    "    retrieval_content:Annotated[list, operator.add]\n",
    "    subquery:str\n",
    "    pdf_path: str\n",
    "    pdf:Annotated[List[str],operator.add]\n",
    "    chunks:Annotated[List[str],operator.add]\n",
    "    query_summary:str\n",
    "\n",
    "   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "def query_expander(state:State):\n",
    "    \n",
    "    user_query = state['query']\n",
    "\n",
    "    human_feedback = state.get('human_feedback_full', \"\")\n",
    "    QUERY_REWRITE_FORMAT = \"\"\"You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. \n",
    "    Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information. Give me 2-3 new quiries. Furthermore, you may be\n",
    "    supplied with user feeback and if so you need to take this into account to \n",
    "    generate queries to the need of the user\n",
    "\n",
    "    Original query: {original_query}\n",
    "\n",
    "    Human Feedback: {human_feedback_full}\n",
    "\n",
    "    Rewritten query:\"\"\"\n",
    "#system_msg = MODEL_SYSTEM_MESSAGE.format(memory=existing_memory_content)\n",
    "\n",
    "    if user_query:\n",
    "        query_rewrite_prompt = QUERY_REWRITE_FORMAT.format(original_query=user_query, human_feedback_full=human_feedback)\n",
    "        subqueries = llm.invoke(query_rewrite_prompt)\n",
    "        subqueries = subqueries.content\n",
    "    else:\n",
    "        raise ValueError(\"Error: Something went wrong\")\n",
    "\n",
    "\n",
    "    return {'subqueries':subqueries}\n",
    "\n",
    "def human_feedback(state: State) -> dict:\n",
    "    human_answer = input(\"Please provide feedback on the query reformulation (type yes if you are fine with everything): \")\n",
    "\n",
    "    if human_answer != 'yes':\n",
    "        return {\"next_node\": \"query_expander\"}  # ✅ Return a dictionary\n",
    "    else:\n",
    "        return {\"next_node\": \"tavily_search\"}  # ✅ Return a dictionary\n",
    "    \n",
    "def decide_next_node(state: dict) -> str:\n",
    "    \"\"\"Determines the next node based on the 'next_node' key in the state.\"\"\"\n",
    "    return state.get('next_node', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "# from langgraph.graph import StateGraph, START\n",
    "\n",
    "# workflow = StateGraph(State)\n",
    "# workflow.add_node(\"query_expander\", query_expander)\n",
    "# workflow.add_node(\"human_feedback\", human_feedback)\n",
    "# workflow.add_edge(START, \"query_expander\")\n",
    "# workflow.add_edge(\"query_expander\", \"human_feedback\")\n",
    "# workflow.add_conditional_edges(\"human_feedback\", should_continue,[\"query_expander\",END])    \n",
    "\n",
    "# memory = MemorySaver()\n",
    "# graph=workflow.compile(interrupt_before=['human_feedback'], checkpointer=memory)\n",
    "# display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts.chat import HumanMessage\n",
    "# initial_input = 'How does machine learning work? Do machines actually learn like humans?'\n",
    "\n",
    "\n",
    "# # Thread\n",
    "# thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# for event in graph.stream({\"query\":initial_input}, thread, stream_mode=\"values\"):\n",
    "#     print(event)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = graph.get_state(thread)\n",
    "# state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We now update the state as if we are the human_feedback node\n",
    "# graph.update_state(thread, {\"human_feedback_full\": \n",
    "#                             \"Make the queries more general such that a fifth grader can understand the question\"}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "#     print(event['subqueries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Optional\n",
    "# from langchain.vectorstores import Chroma\n",
    "\n",
    "# class InfoState(MessagesState):\n",
    "#     retrieval_content:Annotated[list, operator.add]\n",
    "#     subquery:str\n",
    "#     pdf_path: str\n",
    "#     pdf:Annotated[List[str],operator.add]\n",
    "#     chunks:Annotated[List[str],operator.add]\n",
    "#     query_summary:str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "\n",
    "\n",
    "def tavily_search(state:State):\n",
    "    subquery = state.get('subqueries', [\"Are LLMS EVIL?\"])\n",
    "\n",
    "    \n",
    "\n",
    "    TAVILY_SEARCH_PROMPT = \"\"\"You are given the following query:\n",
    "                              {query}\n",
    "                              Your job is to obtain the best information to answer the set of \n",
    "                              questions\"\"\"\n",
    "    TAVILY_API_KEY= os.getenv('TAVILY_API_KEY')\n",
    "\n",
    "    tavily_query_rewrite = TAVILY_SEARCH_PROMPT.format(query=subquery)\n",
    "\n",
    "\n",
    "    tavily_search = TavilySearchResults(api_key=TAVILY_API_KEY)\n",
    "    search_results = tavily_search.invoke(tavily_query_rewrite)\n",
    "\n",
    "\n",
    "\n",
    "    return {\"retrieval_content\":[search_results]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "def pdf_loader(state:State):\n",
    "    pdf_path = state.get(\"pdf_path\",'how__llms__work.pdf')\n",
    "    pdf_loader = PyMuPDFLoader(pdf_path)\n",
    "    pdf_pages=[]\n",
    "    # Extract documents (text)\n",
    "    documents = pdf_loader.load()\n",
    "    for doc in documents:\n",
    "        state['pdf'].append(doc.page_content)  # appending the extracted text to the 'pdf' key in the state\n",
    "    \n",
    "    # Returning the updated state\n",
    "    return {'pdf': state['pdf']}\n",
    "\n",
    "def split_text(state:State):\n",
    "    documents = [Document(page_content=text) for text in state['pdf']]\n",
    "    \n",
    "    # Initialize the text splitter\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\"],\n",
    "        chunk_size=10000,\n",
    "        chunk_overlap=500\n",
    "    )\n",
    "    \n",
    "    # Split the documents into chunks\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    \n",
    "    # Append the chunks to the state['chunks'] list\n",
    "    for chunk in chunks:\n",
    "        state['chunks'].append(chunk)\n",
    "    # Returning the updated state\n",
    "    return {'chunks': state['chunks']}\n",
    "\n",
    "def build_vector_store_retrieval(state: State):\n",
    "    chunks = state.get('chunks', [])  # Use .get() to avoid KeyError if 'chunks' is missing\n",
    "\n",
    "    # Ensure chunks is a list of Document objects\n",
    "    documents = [Document(page_content=str(chunk)) for chunk in chunks]\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    vector_store = Chroma.from_documents(documents, embedding=embeddings)\n",
    "\n",
    "    query = state.get('subqueries', '')  # Avoid KeyError if 'subquery' is missing\n",
    "\n",
    "    search_results = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "    retrieved_content = [doc.page_content for doc in search_results]\n",
    "\n",
    "    return {'retrieval_content': retrieved_content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summary(state: State):\n",
    "    all_retrieved_info = state['retrieval_content']\n",
    "    subqueries = state['subqueries']\n",
    "    formatted_retrieved_content = \"\\n\".join([doc.page_content if isinstance(doc, Document) else str(doc) for doc in all_retrieved_info])\n",
    "    summary_prompt = f\"\"\"\n",
    "    You are an AI assistant tasked with summarizing the following retrieved information:\n",
    "\n",
    "    {formatted_retrieved_content}\n",
    "\n",
    "    Provide a concise and informative summary of the content given the queries: {subqueries}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(summary_prompt)\n",
    "\n",
    "\n",
    "\n",
    "    return {'query_summary': response}\n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builder = StateGraph(InfoState)\n",
    "# builder.add_node('tavily_search', tavily_search)\n",
    "# builder.add_node('pdf_loader', pdf_loader)\n",
    "# builder.add_node('split_text', split_text)\n",
    "# builder.add_node('build_vector_store_retrieval', build_vector_store_retrieval)\n",
    "# builder.add_node('summary', summary)\n",
    "\n",
    "# # Add edges to define the flow of the graph\n",
    "# builder.add_edge(START, 'tavily_search')\n",
    "# builder.add_edge('tavily_search', 'pdf_loader')\n",
    "# builder.add_edge('pdf_loader', 'split_text')\n",
    "# builder.add_edge('split_text', 'build_vector_store_retrieval')\n",
    "# builder.add_edge('build_vector_store_retrieval', 'summary')\n",
    "# builder.add_edge('summary', END)\n",
    "\n",
    "# # Compile the graph\n",
    "# memory = MemorySaver()\n",
    "# interview_graph = builder.compile(checkpointer=memory).with_config(run_name=\"Conduct Retrieval\")\n",
    "\n",
    "# # View the graph (if you want to visualize it)\n",
    "# display(Image(interview_graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_input = {\n",
    "#     'subquery': 'How does Natural language processing work? Do machines actually learn like humans?',\n",
    "#     'pdf_path': \"./how__llms__work.pdf\",\n",
    "\n",
    "# }\n",
    "# thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "# # Run the graph with the state_input\n",
    "# result = interview_graph.invoke(state_input,thread)\n",
    "\n",
    "# # The result will contain the output from the final node in the graph (e.g., 'summary')\n",
    "# print(result['query_summary'].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FinalState(MessagesState):\n",
    "#     final_composed_summary:Annotated[List[str],operator.add]\n",
    "#     summary:str\n",
    "#     human_feedback_full:str\n",
    "#     subqueries: Annotated[List[str], operator.add]\n",
    "#     pdf_path:str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.constants import Send\n",
    "from langgraph.constants import START, END\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node('query_expander', query_expander)\n",
    "builder.add_node('human_feedback', human_feedback)\n",
    "builder.add_node('tavily_search', tavily_search)\n",
    "builder.add_node('pdf_loader', pdf_loader)\n",
    "builder.add_node('split_text', split_text)\n",
    "builder.add_node('build_vector_store_retrieval', build_vector_store_retrieval)\n",
    "builder.add_node('summary', summary)\n",
    "\n",
    "builder.add_edge(START,'query_expander')\n",
    "builder.add_edge('query_expander','human_feedback')\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\",\n",
    "    decide_next_node,   # Reference the function here — no need to define it as a node!\n",
    "    [\"query_expander\", \"tavily_search\"])\n",
    "builder.add_edge(\"tavily_search\",'pdf_loader')\n",
    "builder.add_edge(\"pdf_loader\",'split_text')\n",
    "builder.add_edge(\"split_text\",'build_vector_store_retrieval')\n",
    "builder.add_edge(\"build_vector_store_retrieval\",'summary')\n",
    "builder.add_edge(\"summary\",END)\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "compiled_builder = builder.compile(checkpointer=memory)\n",
    "\n",
    "# Display the graph (optional)\n",
    "display(Image(compiled_builder.get_graph(xray=1).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query_expander': {'subqueries': '1. \"Can you explain the technology and functionality behind a Large Language Model (LLM), including its architecture, training process, and application areas?\"\\n2. \"What are the key components and workings of a Large Language Model (LLM)? Please include details about data processing, model size, and generation of human-like text.\"\\n3. \"Considering my interest in AI, provide an in-depth explanation of Large Language Models (LLMs), focusing on their design, operation, and use cases, as well as their limitations and future potential.\"'}}\n",
      "{'human_feedback': None}\n",
      "{'tavily_search': {'retrieval_content': [\"HTTPError('400 Client Error: Bad Request for url: https://api.tavily.com/search')\"]}}\n",
      "{'pdf_loader': {'pdf': ['A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,\\nNick Barnesi, Ajmal Mianj\\naThe University of Sydney, Sydney, Australia\\nbUniversity of Engineering and Technology (UET), Lahore, Pakistan\\ncThe Chinese University of Hong Kong (CUHK), HKSAR, China\\ndUniversity of Technology Sydney (UTS), Sydney, Australia\\neCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\\nfKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\\ngSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\\nhThe University of Melbourne (UoM), Melbourne, Australia\\niAustralian National University (ANU), Canberra, Australia\\njThe University of Western Australia (UWA), Perth, Australia\\nAbstract\\nLarge Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\\nbeyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in\\nLLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\\nthe rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise\\nyet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on a broad\\nrange of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts\\nalong with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only a\\nsystematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive,\\ninformative summaries of the existing works to advance the LLM research.\\nKeywords:\\nLarge Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\\n1. Introduction\\nLanguage plays a fundamental role in facilitating commu-\\nnication and self-expression for humans and their interaction\\nwith machines. The need for generalized models stems from\\nthe growing demand for machines to handle complex language\\ntasks, including translation, summarization, information re-\\ntrieval, conversational interactions, etc. Recently, significant\\nbreakthroughs have been witnessed in language models, pri-\\nmarily attributed to transformers [1], increased computational\\ncapabilities, and the availability of large-scale training data.\\nThese developments have brought about a revolutionary trans-\\nformation by enabling the creation of LLMs that can approxi-\\nmate human-level performance on various tasks [2, 3]. Large\\n∗Equal contribution\\nEmail addresses: humza_naveed@yahoo.com (Humza Naveed),\\naukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi\\nQiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),\\nmuhammad.usman@kfupm.edu.sa (Muhammad Usman),\\nnaveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\\nnick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\\n(Ajmal Mian)\\nFigure 1: The trend of papers released over the years containing keywords\\n\"Large Language Model\", \"Large Language Model + Fine-Tuning\", and \"Large\\nLanguage Model + Alignment\".\\nPreprint submitted to Elsevier\\nOctober 18, 2024\\narXiv:2307.06435v10  [cs.CL]  17 Oct 2024', '2019\\nT5 (Oct)\\nGPT-3 (May)\\nWebGPT (Dec)\\nOPT-IML\\nTK-Instruct (May)\\nmT0 (Dec)\\nWizard-LM\\nVicuna\\nAlpaca (Mar)\\nHuaTuo (Apr)\\nKoala (May)\\nWizard-Coder (Jun)\\nGoat\\nPanGu-α (Apr)\\nCPM-2 (Jun)\\nGPT-NeoX-20B (Apr)\\nCodeGen (Mar) \\nGalactica (Nov)\\nGLM (Oct)\\nOPT\\nUL2 (May)\\nLLaMA (Feb)\\nLLaMA 2 (Jul)\\nMPT (Jun)\\nCodeT5+\\nCode Llama (Aug)\\nStarCoder\\nXuan Yuan 2.0 (May)\\n2020\\n2021\\n2022\\n2023\\n2024\\nmT5 (Oct)\\nHyperCLOVA (Sep)\\nERNIE 3.0\\nCodex (Jul)\\nJurassic-1 (Aug)\\nYuan 1.0 (Oct)\\nGopher (Dec)\\nERNIE 3.0 Titan\\nGLaM\\nLaMDA\\nT0 (Oct)\\nChatGPT (Nov)\\nSparrow (Sep)\\nFLAN-U-PaLM (Oct)\\nBard (Oct)\\nMT-NLG (Jan)\\nAlphaCode (Feb)\\nChinchilla (Mar)\\nPaLM (Apr)\\nU-PALM (Oct)\\nBLOOM (Nov)\\nAlexaTM (Aug)\\nPaLM2 (May)\\nGPT-4\\nPanGu-Σ (Mar)\\nBloombergGPT\\nClaude\\nGemini (Dec)\\nDeepSeek (Jan)\\nLLaMA 3 \\nGrok-1 (Mar)\\nSnowflake Arctic (Apr)\\nDeepSeek-V2 (May)\\nMixtral 8x22B\\nNemotron (Feb)\\nGPT-4o (May)\\nOpenAI o1 (Sep)\\nGemini-1.5 (Feb)\\nGrok-1.5 (Apr)\\nFigure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models. Models\\non the upper half signify open-source availability, whereas those on the bottom are closed-source. The chart illustrates the increasing trend towards instruction-tuned\\nand open-source models, highlighting the evolving landscape and trends in natural language processing research.\\nLanguage Models (LLMs) have emerged as cutting-edge arti-\\nficial intelligence systems that can process and generate text\\nwith coherent communication [4] and generalize to multiple\\ntasks [5, 6].\\nThe historical progress in natural language processing (NLP)\\nevolved from statistical to neural language modeling and then\\nfrom pre-trained language models (PLMs) to LLMs.\\nWhile\\nconventional language modeling (LM) trains task-specific mod-\\nels in supervised settings, PLMs are trained in a self-supervised\\nsetting on a large corpus of text [7, 8, 9] with the aim of learning\\na generic representation that is shareable among various NLP\\ntasks. After fine-tuning for downstream tasks, PLMs surpass\\nthe performance gains of traditional language modeling (LM).\\nThe larger PLMs bring more performance gains, which has led\\nto the transitioning of PLMs to LLMs by significantly increas-\\ning model parameters (tens to hundreds of billions) [10] and\\ntraining dataset (many GBs and TBs) [10, 11]. Following this\\ndevelopment, numerous LLMs have been proposed in the lit-\\nerature [10, 11, 12, 6, 13, 14, 15]. An increasing trend in the\\nnumber of released LLMs and names of a few significant LLMs\\nproposed over the years are shown in Fig 1 and Fig 2, respec-\\ntively.\\nThe early work on LLMs, such as T5 [10] and mT5 [11] em-\\nployed transfer learning until GPT-3 [6] showed LLMs are\\nzero-shot transferable to downstream tasks without fine-tuning.\\nLLMs accurately respond to task queries when prompted with\\ntask descriptions and examples. However, pre-trained LLMs\\nfail to follow user intent and perform worse in zero-shot set-\\ntings than in few-shot.\\nFine-tuning them with task instruc-\\ntions data [16, 17, 18, 19] and aligning with human prefer-\\nences [20, 21] enhances generalization to unseen tasks, im-\\nproving zero-shot performance significantly and reducing mis-\\naligned behavior.\\nIn addition to better generalization and domain adaptation,\\nLLMs appear to have emergent abilities, such as reasoning,\\nplanning, decision-making, in-context learning, answering in\\nzero-shot settings, etc.\\nThese abilities are known to be ac-\\nquired by them due to their gigantic scale even when the pre-\\ntrained LLMs are not trained specifically to possess these at-\\ntributes [22, 23, 24]. Such abilities have led LLMs to be widely\\nadopted in diverse settings, including multi-modal, robotics,\\ntool manipulation, question answering, autonomous agents, etc.\\nVarious improvements have also been suggested in these areas\\neither by task-specific training [25, 26, 27, 28, 29, 30, 31] or\\nbetter prompting [32].\\nThe LLMs abilities to solve diverse tasks with human-level\\nperformance come at the cost of slow training and inference,\\nextensive hardware requirements, and higher running costs.\\nSuch requirements have limited their adoption and opened up\\nopportunities to devise better architectures [15, 33, 34, 35]\\nand training strategies [36, 37, 21, 38, 39, 40, 41].\\nParam-\\neter efficient tuning [38, 41, 40], pruning [42, 43], quantiza-\\ntion [44, 45], knowledge distillation, and context length inter-\\npolation [46, 47, 48, 49] among others are some of the methods\\nwidely studied for efficient LLM utilization.\\nDue to the success of LLMs on a wide variety of tasks, the\\nresearch literature has recently experienced a large influx of\\nLLM-related contributions.\\nResearchers have organized the\\nLLMs literature in surveys [50, 51, 52, 53], and topic-specific\\nsurveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our\\ncontribution focuses on providing a comprehensive yet concise\\noverview of the general direction of LLM research. This arti-\\ncle summarizes architectural and training details of pre-trained\\nLLMs and delves deeper into the details of concepts like fine-\\ntuning, multi-modal LLMs, augmented LLMs, datasets, eval-\\nuation, applications, challenges, and others to provide a self-\\ncontained comprehensive overview. Our key contributions are\\nsummarized as follows.\\n• We present a survey on the developments in LLM research,\\nproviding a concise, comprehensive overview of the direc-\\ntion.\\n• We present extensive summaries of pre-trained models that\\ninclude fine-grained details of architecture and training de-\\ntails.\\n• We summarize major findings of the popular contributions\\nand provide a detailed discussion on the key design and\\ndevelopment aspects of LLMs to help practitioners effec-\\ntively leverage this technology.\\n• In this self-contained article, we cover a range of con-\\ncepts to present the general direction of LLMs compre-\\nhensively, including background, pre-training, fine-tuning,\\n2', 'Figure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1. Pre-Training 2. Fine-Tuning 3. Efficient 4. Inference 5. Evaluation 6. Applications\\n7. Challenges\\nmulti-modal LLMs, augmented LLMs, LLMs-powered\\nagents, datasets, evaluation, etc.\\nWe loosely follow the existing terminology to ensure a stan-\\ndardized outlook of this research direction. For instance, fol-\\nlowing [50], our survey discusses pre-trained LLMs with 10B\\nparameters or more. We refer the readers interested in smaller\\npre-trained models to [51, 52, 53].\\nThe organization of this paper is as follows. Section 2 discusses\\nthe background of LLMs. Section 3 focuses on LLMs overview,\\narchitectures, training pipelines and strategies, fine-tuning, and\\nutilization in different domains. Section 4 highlights the config-\\nuration and parameters that play a crucial role in the function-\\ning of these models. Summary and discussions are presented\\nin section 3.8. The LLM training and evaluation, datasets, and\\nbenchmarks are discussed in section 5, followed by challenges\\nand future directions, and conclusion in sections 7 and 8, re-\\nspectively.\\n3', '2. Background\\nWe provide the relevant background to understand the fun-\\ndamentals related to LLMs in this section. We briefly discuss\\nnecessary components in LLMs and refer the readers interested\\nin details to the original works.\\n2.1. Tokenization\\nTokenization [59] is an essential pre-processing step in\\nLLM training that parses the text into non-decomposing units\\ncalled tokens. Tokens can be characters, subwords [60], sym-\\nbols [61], or words, depending on the tokenization process.\\nSome of the commonly used tokenization schemes in LLMs\\ninclude wordpiece [62], byte pair encoding (BPE) [61], and un-\\nigramLM [60]. Readers are encouraged to refer to [63] for a\\ndetailed survey.\\n2.2. Encoding Positions\\nThe transformer processes input sequences in parallel and\\nindependently of each other.\\nMoreover, the attention mod-\\nule in the transformer does not capture positional information.\\nAs a result, positional encodings were introduced in trans-\\nformer [64], where a positional embedding vector is added to\\nthe token embedding. Variants of positional embedding include\\nabsolute, relative, or learned positional encodings. Within rel-\\native encoding, Alibi and RoPE are two widely used positional\\nembeddings in LLMs.\\nAlibi [65]: It subtracts a scalar bias from the attention score\\nthat increases with the distance between token positions. This\\nfavors using recent tokens for attention.\\nRoPE [66]: It rotates query and key representations at an an-\\ngle proportional to the token absolute position in the input\\nsequence, resulting in a relative positional encoding scheme\\nwhich decays with the distance between the tokens.\\n2.3. Attention in LLMs\\nAttention assigns weights to input tokens based on impor-\\ntance so that the model gives more emphasis to relevant tokens.\\nAttention in transformers [64] calculates query, key, and value\\nmappings for input sequences, where the attention score is\\nobtained by multiplying the query and key, and later used to\\nweight values. We discuss different attention strategies used in\\nLLMs below.\\nSelf-Attention [64]: Calculates attention using queries, keys,\\nand values from the same block (encoder or decoder).\\nCross Attention: It is used in encoder-decoder architectures,\\nwhere encoder outputs are the queries, and key-value pairs\\ncome from the decoder.\\nSparse Attention [67]: Self-attention has O(n2) time complex-\\nity which becomes infeasible for large sequences. To speed\\nup the computation, sparse attention [67] iteratively calculates\\nattention in sliding windows for speed gains.\\nFlash Attention [68]: Memory access is the major bottleneck\\nin calculating attention using GPUs.\\nTo speed up, flash\\nattention employs input tiling to minimize the memory reads\\nand writes between the GPU high bandwidth memory (HBM)\\nand the on-chip SRAM.\\n2.4. Activation Functions\\nThe activation functions serve a crucial role in the curve-\\nfitting abilities of neural networks [69]. We discuss activation\\nfunctions used in LLMs in this section.\\nReLU [70]: The Rectified linear unit (ReLU) is defined as:\\nReLU(x) = max(0, x)\\n(1)\\nGeLU [71]: The Gaussian Error Linear Unit (GeLU) is the\\ncombination of ReLU, dropout [72] and zoneout [73].\\nGLU variants [74]: The Gated Linear Unit [75] is a neural\\nnetwork layer that is an element-wise product (⊗) of a linear\\ntransformation and a sigmoid transformed (σ) linear projection\\nof the input given as:\\nGLU(x, W, V, b, c) = (xW + b) ⊗σ(xV + c),\\n(2)\\nwhere X is the input of layer and l, W, b, V and c are learned\\nparameters. Other GLU variants [74] used in LLMs are:\\nReGLU(x, W, V, b, c) = max(0, xW + b)⊗,\\nGEGLU(x, W, V, b, c) = GELU(xW + b) ⊗(xV + c),\\nS wiGLU(x, W, V, b, c, β) = S wishβ(xW + b) ⊗(xV + c).\\n2.5. Layer Normalization\\nLayer normalization leads to faster convergence and is an in-\\ntegrated component of transformers [64]. In addition to Layer-\\nNorm [76] and RMSNorm [77], LLMs use pre-layer normal-\\nization [78], applying it before multi-head attention (MHA).\\nPre-norm is shown to provide training stability in LLMs. An-\\nother normalization variant, DeepNorm [79] fixes the issue with\\nlarger gradients in pre-norm.\\n2.6. Distributed LLM Training\\nThis section describes distributed LLM training approaches\\nbriefly. More details are available in [13, 37, 80, 81].\\nData Parallelism: Data parallelism replicates the model on\\nmultiple devices where data in a batch gets divided across de-\\nvices. At the end of each training iteration weights are synchro-\\nnized across all devices.\\nTensor Parallelism: Tensor parallelism shards a tensor compu-\\ntation across devices. It is also known as horizontal parallelism\\nor intra-layer model parallelism.\\nPipeline Parallelism: Pipeline parallelism shards model layers\\nacross different devices. This is also known as vertical paral-\\nlelism.\\nModel Parallelism: A combination of tensor and pipeline par-\\nallelism is known as model parallelism.\\n3D Parallelism: A combination of data, tensor, and model par-\\nallelism is known as 3D parallelism.\\nOptimizer Parallelism: Optimizer parallelism also known as\\nzero redundancy optimizer [37] implements optimizer state\\npartitioning, gradient partitioning, and parameter partitioning\\nacross devices to reduce memory consumption while keeping\\nthe communication costs as low as possible.\\n4', '2.7. Libraries\\nSome commonly used libraries for LLMs training are:\\nTransformers [82]: The library provides access to various pre-\\ntrained transformer models with APIs to train, fine-tune, infer,\\nand develop custom models.\\nDeepSpeed [36]: A library for scalable distributed training and\\ninference of deep learning models.\\nMegatron-LM [80]: It provides GPU-optimized techniques for\\nlarge-scale training of LLMs.\\nJAX [83]: A Python library for high-performance numerical\\ncomputing and scaleable machine learning. It can differenti-\\nate native Python and NumPy functions and execute them on\\nGPUs.\\nColossal-AI [84]: A collection of components to write dis-\\ntributed deep learning models.\\nBMTrain [81]: A library to write efficient stand-alone LLMs\\ntraining code.\\nFastMoE [85]:\\nProvides API to build mixture-of-experts\\n(MoE) model in PyTorch.\\nMindSpore [86]: A deep learning training and inference frame-\\nwork extendable to mobile, edge, and cloud computing.\\nPyTorch [87]: A framework developed by Facebook AI Re-\\nsearch lab (FAIR) to build deep learning models. The main\\nfeatures of PyTorch include a dynamic computation graph and\\na pythonic coding style.\\nTensorflow [88]:\\nA deep learning framework written by\\nGoogle. The key features of TensorFlow are graph-based com-\\nputation, eager execution, scalability, etc.\\nMXNet [89]: Apache MXNet is a deep learning framework\\nwith support to write programs in multiple languages, includ-\\ning, Python, C++, Scala, R, etc. It also provides support for\\ndynamic and static computation graphs.\\n2.8. Data PreProcessing\\nThis section briefly summarizes data preprocessing tech-\\nniques used in LLMs training.\\nQuality Filtering: For better results, training data quality is\\nessential. Some approaches to filtering data are: 1) classifier-\\nbased and 2) heuristics-based.\\nClassifier-based approaches\\ntrain a classifier on high-quality data and predict the quality of\\ntext for filtering, whereas heuristics-based employ some rules\\nfor filtering like language, metrics, statistics, and keywords.\\nData Deduplication: Duplicated data can affect model per-\\nformance and increase data memorization; therefore, to train\\nLLMs, data deduplication is one of the preprocessing steps.\\nThis can be performed at multiple levels, like sentences,\\ndocuments, and datasets.\\nPrivacy Reduction: Most of the training data for LLMs is\\ncollected through web sources.\\nThis data contains private\\ninformation; therefore, many LLMs employ heuristics-based\\nmethods to filter information such as names, addresses, and\\nphone numbers to avoid learning personal information.\\n2.9. Architectures\\nHere we discuss the variants of the transformer architectures\\nused in LLMs. The difference arises due to the application of\\nFigure 4: An example of attention patterns in language models, image is taken\\nfrom [93].\\nFigure 5: An example of language model training objectives, image from [93].\\nthe attention and the connection of transformer blocks. An il-\\nlustration of attention patterns of these architectures is shown\\nin Figure 4.\\nEncoder Decoder: This architecture processes inputs through\\nthe encoder and passes the intermediate representation to the\\ndecoder to generate the output.\\nHere, the encoder sees the\\ncomplete sequence utilizing self-attention whereas the decoder\\nprocesses the sequence one after the other with implementing\\ncross-attention.\\nCausal Decoder: A type of architecture that does not have an\\nencoder and processes and generates output using a decoder,\\nwhere the predicted token depends only on the previous time\\nsteps.\\nPrefix Decoder: It is also known as a non-causal decoder,\\nwhere the attention calculation is not strictly dependent on the\\npast information and the attention is bidirectional. An example\\nof a non-causal attention mask is shown in Figure 4.\\nMixture-of-Experts: It is a variant of transformer architecture\\nwith parallel independent experts and a router to route tokens\\nto experts. These experts are feed-forward layers after the at-\\ntention block [90]. Mixture-of-Experts (MoE) is an efficient\\nsparse architecture that offers comparable performance to dense\\nmodels and allows increasing the model size without increas-\\ning the computational cost by activating only a few experts at a\\ntime [91, 92].\\n2.10. Pre-Training Objectives\\nThis section describes LLMs pre-training objectives.\\nFor\\nmore details see the paper [93].\\nFull Language Modeling: An autoregressive language model-\\ning objective where the model is asked to predict future tokens\\ngiven the previous tokens, an example is shown in Figure 5.\\nPrefix Language Modeling: A non-causal training objective,\\nwhere a prefix is chosen randomly and only remaining target\\ntokens are used to calculate the loss. An example is shown in\\nFigure 5.\\n5', 'Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting/utilization. Prompting LLMs to generate responses is possible at\\ndifferent training stages like pre-training, instruction-tuning, or alignment tuning. “RL” stands for reinforcement learning, “RM” represents reward-modeling, and\\n“RLHF” represents reinforcement learning with human feedback.\\nMasked Language Modeling: In this training objective, tokens\\nor spans (a sequence of tokens) are masked randomly and the\\nmodel is asked to predict masked tokens given the past and\\nfuture context. An example is shown in Figure 5.\\nUnified Language Modeling: Unified language modeling [94]\\nis a combination of causal, non-causal, and masked language\\ntraining objectives. Here in masked language modeling, the\\nattention is not bidirectional but unidirectional, attending either\\nleft-to-right or right-to-left context.\\n2.11. LLMs Scaling Laws\\nScaling laws study the optimal combination of model param-\\neters, dataset size, and computational resources that predict the\\nimprovement in the model performance. It has been shown\\nthat the loss scales according to the power-law with model size,\\ndataset size, and compute resources [95]. This study suggests\\nlarger models are more important than big data for better perfor-\\nmance. Another variant of scaling law [96] suggests the model\\nsize and the number of training tokens should be scaled equally.\\n2.12. LLMs Adaptation Stages\\nThis section discusses the fundamentals of LLMs adaptation\\nstages, from pre-training to fine-tuning for downstream tasks\\nand utilization. An example of different training stages and in-\\nference in LLMs is shown in Figure 6. In this paper, we refer\\nto alignment-tuning as aligning with human preferences, while\\noccasionally the literature uses the term alignment for different\\npurposes.\\n2.12.1. Pre-Training\\nIn the very first stage, the model is trained in a self-\\nsupervised manner on a large corpus to predict the next to-\\nkens given the input. The design choices of LLMs vary from\\nencoder-decoder to decoder-only architectures with different\\nbuilding blocks and loss functions in sections 2.5, 2.4, 2.10.\\n2.12.2. Fine-Tuning\\nThere are different styles to fine-tune an LLM. This section\\nbriefly discusses fine-tuning approaches.\\nTransfer Learning: The pre-trained LLMs perform well for\\nvarious tasks [6, 15]. However, to improve the performance for\\n6', 'a downstream task, pre-trained models are fine-tuned with the\\ntask-specific data [10, 11], known as transfer learning.\\nInstruction-tuning: To enable a model to respond to user\\nqueries effectively, the pre-trained model is fine-tuned on in-\\nstruction formatted data i.e., instruction and an input-output\\npair. Instructions generally comprise multi-task data in plain\\nnatural language, guiding the model to respond according to the\\nprompt and the input. This type of fine-tuning improves zero-\\nshot generalization and downstream task performance. Details\\non formatting instruction data and its various styles are avail-\\nable in [16, 50, 97].\\nAlignment-tuning: LLMs are prone to generating false, biased,\\nand harmful text. To make them helpful, honest, and harmless,\\nmodels are aligned using human feedback. Alignment involves\\nasking LLMs to generate unexpected responses and then updat-\\ning their parameters to avoid such responses [20, 21, 98].\\nIt ensures LLMs operate according to human intentions and\\nvalues. A model is defined to be an “aligned” model if the\\nmodel fulfills three criteria of helpful, honest, and harmless or\\n“HHH” [99].\\nResearchers employ reinforcement learning with human feed-\\nback (RLHF) [100] for model alignment. In RLHF, a fine-tuned\\nmodel on demonstrations is further trained with reward model-\\ning (RM) and reinforcement learning (RL), shown in Figure 6.\\nBelow we briefly discuss RM and RL pipelines in RLHF.\\nReward modeling: trains a model to rank generated responses\\naccording to human preferences using a classification objec-\\ntive. To train the classifier humans annotate LLMs generated\\nresponses based on the HHH criteria.\\nReinforcement learning: in combination with the reward model\\nis used for alignment in the next stage. The previously trained\\nreward model ranks LLM-generated responses into preferred\\nvs. non-preferred, which is used to align the model with proxi-\\nmal policy optimization (PPO). This process repeats iteratively\\nuntil convergence.\\n2.12.3. Prompting/Utilization\\nPrompting is a method to query trained LLMs for generating\\nresponses, as illustrated in Figure 6. LLMs can be prompted in\\nvarious prompt setups, where they can be adapted to the instruc-\\ntions without fine-tuning and in other cases with fine-tuning on\\ndata containing different prompt styles [16, 101, 102]. A good\\nguide on prompt engineering is available at [32]. Below, we\\nwill discuss various widely used prompt setups.\\nZero-Shot Prompting: LLMs are zero-shot learners and ca-\\npable of answering queries never seen before. This style of\\nprompting requires LLMs to answer user questions without see-\\ning any examples in the prompt.\\nIn-context Learning: Also known as few-shot learning, here,\\nmultiple input-output demonstration pairs are shown to the\\nmodel to generate the desired response. This adaptation style\\nis also called few-shot learning. A discussion on formatting in-\\ncontext learning (ICL) templates is available in [54, 50, 18, 16].\\nReasoning in LLMs: LLMs are zero-shot reasoners and can\\nbe provoked to generate answers to logical problems, task\\nplanning, critical thinking, etc. with reasoning.\\nGenerating\\nreasons is possible only by using different prompting styles,\\nwhereas to improve LLMs further on reasoning tasks many\\nmethods [16, 97] train them on reasoning datasets. We discuss\\nvarious prompting techniques for reasoning below.\\nChain-of-Thought (CoT): A special case of prompting where\\ndemonstrations contain reasoning information aggregated with\\ninputs and outputs so that the model generates outcomes with\\nstep-by-step reasoning. More details on CoT prompts are avail-\\nable in [55, 103, 101].\\nSelf-Consistency:\\nImproves CoT performance by generat-\\ning multiple responses and selecting the most frequent an-\\nswer [104].\\nTree-of-Thought (ToT): Explores multiple reasoning paths\\nwith possibilities to look ahead and backtrack for problem-\\nsolving [105].\\nSingle-Turn Instructions: In this prompting setup, LLMs are\\nqueried only once with all the relevant information in the\\nprompt. LLMs generate responses by understanding the con-\\ntext either in a zero-shot or few-shot setting.\\nMulti-Turn Instructions: Solving a complex task requires mul-\\ntiple interactions with LLMs, where feedback and responses\\nfrom the other tools are given as input to the LLM for the next\\nrounds. This style of using LLMs in the loop is common in\\nautonomous agents.\\n3. Large Language Models\\nThis section reviews LLMs, briefly describing their architec-\\ntures, training objectives, pipelines, datasets, and fine-tuning\\ndetails.\\n3.1. Pre-Trained LLMs\\nHere, we provide summaries of various well-known pre-\\ntrained LLMs with significant discoveries, changing the course\\nof research and development in NLP. These LLMs have consid-\\nerably improved the performance in NLU and NLG domains,\\nand are widely fine-tuned for downstream tasks. Moreover, We\\nalso identify key findings and insights of pre-trained LLMs in\\nTable 1 and 2 that improve their performance.\\n3.1.1. General Purpose\\nT5 [10]: An encoder-decoder model employing a unified text-\\nto-text training for all NLP problems is shown in Figure 7. T5\\nplaces layer normalization outside the residual path in a conven-\\ntional transformer model [64]. It uses masked language mod-\\neling as a pre-training objective where spans (consecutive to-\\nkens) are replaced with a single mask instead of separate masks\\nfor each token. This type of masking speeds up the training as\\nit produces shorter sequences. After pre-training, the model is\\nfine-tuned using adapter layers [106] for downstream tasks.\\nGPT-3 [6]: The GPT-3 architecture is the same as the GPT-\\n2 [5] but with dense and sparse attention in transformer layers\\nsimilar to the Sparse Transformer [67]. It shows that large mod-\\nels can train on larger batch sizes with a lower learning rate to\\ndecide the batch size during training, GPT-3 uses the gradient\\nnoise scale as in [107]. Overall, GPT-3 increases model param-\\neters to 175B showing that the performance of large language\\n7', 'Figure 7: Unified text-to-text training example, source image from [10].\\nFigure 8: The image is the article of [108], showing an example of PanGu-α\\narchitecture.\\nmodels improves with the scale and is competitive with the fine-\\ntuned models.\\nmT5 [11]: A multilingual T5 model [10] trained on the mC4\\ndataset with 101 languages. The dataset is extracted from the\\npublic common crawl scrape. The model uses a larger vocab-\\nulary size of 250,000 to cover multiple languages. To avoid\\nover-fitting or under-fitting for a language, mT5 employs a data\\nsampling procedure to select samples from all languages. The\\npaper suggests using a small amount of pre-training datasets,\\nincluding all languages when fine-tuning for a task using En-\\nglish language data. This allows the model to generate correct\\nnon-English outputs.\\nPanGu-α [108]: An autoregressive model that has a query\\nlayer at the end of standard transformer layers, example shown\\nin Figure 8, to predict the next token. Its structure is similar to\\nthe transformer layer but with an additional embedding for the\\nnext position in the attention mechanism, given in Eq. 3.\\na = pnWq\\nhWk\\nhTHT\\nL\\n(3)\\nCPM-2 [12]: Cost-efficient Pre-trained language Models\\n(CPM-2) pre-trains bilingual (English and Chinese) 11B and\\n198B mixture-of-experts (MoE) models on the WuDaoCor-\\npus [109] dataset. The tokenization process removes “_” white\\nspace tokens in the sentencepiece tokenizer. The models are\\ntrained with knowledge inheritance, starting with only the Chi-\\nnese language in the first stage and then adding English and\\nChinese data. This trained model gets duplicated multiple times\\nto initialize the 198B MoE model. Moreover, to use the model\\nfor downstream tasks, CPM-2 experimented with both com-\\nplete fine-tuning and prompt fine-tuning as in [40] where only\\nprompt-related parameters are updated by inserting prompts at\\nvarious positions, front, middle, and back. CPM-2 also pro-\\nposes the INFMOE, a memory-efficient framework with a strat-\\negy to dynamically offload parameters to the CPU for inference\\nat a 100B scale. It overlaps data movement with inference com-\\nputation for lower inference time.\\nERNIE 3.0 [110]: ERNIE 3.0 takes inspiration from multi-\\ntask learning to build a modular architecture using Transformer-\\nXL [111] as the backbone. The universal representation mod-\\nule is shared by all the tasks, which serve as the basic block\\nfor task-specific representation modules, which are all trained\\njointly for natural language understanding, natural language\\ngeneration, and knowledge extraction. This LLM is primar-\\nily focused on the Chinese language. It claims to train on the\\nlargest Chinese text corpora for LLM training, and achieved\\nstate-of-the-art in 54 Chinese NLP tasks.\\nJurassic-1 [112]: A pair of auto-regressive language mod-\\nels, including a 7B-parameter J1-Large model and a 178B-\\nparameter J1-Jumbo model.\\nThe training vocabulary of\\nJurassic-1 comprise word pieces, complete words, and multi-\\nword expressions without any word boundaries, where possible\\nout-of-vocabulary instances are interpreted as Unicode bytes.\\nCompared to the GPT-3 counterparts, the Jurassic-1 models\\napply a more balanced depth-to-width self-attention architec-\\nture [113] and an improved tokenizer for a faster prediction\\nbased on broader resources, achieving a comparable perfor-\\nmance in zero-shot learning tasks and a superior performance in\\nfew-shot learning tasks given the ability to feed more examples\\nas a prompt.\\nHyperCLOVA [114]: A Korean language model with GPT-3\\narchitecture.\\nYuan 1.0 [115]: Trained on a Chinese corpus with 5TB of\\nhigh-quality text collected from the Internet. A Massive Data\\nFiltering System (MDFS) built on Spark is developed to pro-\\ncess the raw data via coarse and fine filtering techniques. To\\nspeed up the training of Yuan 1.0 to save energy expenses and\\ncarbon emissions, various factors that improve the performance\\nof distributed training are incorporated in architecture and train-\\ning: like increasing the hidden state size improves pipeline and\\ntensor parallelism performance, larger micro batches improve\\npipeline parallelism performance, and larger global batch size\\nimprove data parallelism performance. In practice, the Yuan 1.0\\nmodel performs well on text classification, Winograd Schema,\\nnatural language inference, and reading comprehension tasks.\\nGopher [116]: The Gopher family of models ranges from\\n44M to 280B parameters in size to study the effect of scale\\non the LLMs performance. The 280B model beats GPT-3 [6],\\nJurrasic-1 [112], MT-NLG [117], and others on 81% of the\\nevaluated tasks.\\nERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan extends ERNIE 3.0\\nby training a larger model with 26x the number of parameters\\nof the latter. This bigger model outperformed other state-of-the-\\nart models in 68 NLP tasks. LLMs produce text with incorrect\\nfacts. In order to have control of the generated text with fac-\\ntual consistency, ERNIE 3.0 Titan adds another task, Credible\\nand Controllable Generations, to its multi-task learning setup.\\n8', 'It introduces additional self-supervised adversarial and control-\\nlable language modeling losses to the pre-training step, which\\nenables ERNIE 3.0 Titan to beat other LLMs in their manually\\nselected Factual QA task set evaluations.\\nGPT-NeoX-20B [118]: An auto-regressive model that largely\\nfollows GPT-3 with a few deviations in architecture design,\\ntrained on the Pile dataset without any data deduplication. GPT-\\nNeoX has parallel attention and feed-forward layers in a trans-\\nformer block, given in Eq. 4, that increases throughput by 15%.\\nIt uses rotary positional embedding [66], applying it to only\\n25% of embedding vector dimension as in [119]. This reduces\\nthe computation without performance degradation. As opposed\\nto GPT-3, which uses dense and sparse layers, GPT-NeoX-20B\\nuses only dense layers. The hyperparameter tuning at this scale\\nis difficult; therefore, the model chooses hyperparameters from\\nthe method [6] and interpolates values between 13B and 175B\\nmodels for the 20B model. The model training is distributed\\namong GPUs using both tensor and pipeline parallelism.\\nx + Attn(LN1(x)) + FF(LN2(x))\\n(4)\\nOPT [14]: It is a clone of GPT-3, developed to open-source\\na model that replicates GPT-3 performance. Training of OPT\\nemploys dynamic loss scaling [120] and restarts from an earlier\\ncheckpoint with a lower learning rate whenever loss divergence\\nis observed. Overall, the performance of OPT-175B models is\\ncomparable to the GPT3-175B model.\\nBLOOM [13]: A causal decoder model trained on the ROOTS\\ncorpus to open-source an LLM. The architecture of BLOOM is\\nshown in Figure 9, with differences like ALiBi positional em-\\nbedding, an additional normalization layer after the embedding\\nlayer as suggested by the bitsandbytes1 library. These changes\\nstabilize training with improved downstream performance.\\nGLaM [91]: Generalist Language Model (GLaM) represents a\\nfamily of language models using a sparsely activated decoder-\\nonly mixture-of-experts (MoE) structure [121, 90].\\nTo gain\\nmore model capacity while reducing computation, the experts\\nare sparsely activated where only the best two experts are used\\nto process each input token. The largest GLaM model, GLaM\\n(64B/64E), is about 7× larger than GPT-3 [6], while only part of\\nthe parameters are activated per input token. The largest GLaM\\n(64B/64E) model achieves better overall results as compared\\nto GPT-3 while consuming only one-third of GPT-3’s training\\nenergy.\\nMT-NLG [117]: A 530B causal decoder based on the GPT-\\n2 architecture that has roughly 3× GPT-3 model parameters.\\nMT-NLG is trained on filtered high-quality data collected from\\nvarious public datasets and blends various types of datasets in a\\nsingle batch, which beats GPT-3 on several evaluations.\\nChinchilla [96]: A causal decoder trained on the same dataset\\nas the Gopher [116] but with a little different data sampling\\ndistribution (sampled from MassiveText). The model architec-\\nture is similar to the one used for Gopher, with the exception of\\nAdamW optimizer instead of Adam. Chinchilla identifies the\\n1https://github.com/TimDettmers/bitsandbytes\\nFigure 9: The BLOOM architecture example sourced from [13].\\nrelationship that model size should be doubled for every dou-\\nbling of training tokens. Over 400 language models ranging\\nfrom 70 million to over 16 billion parameters on 5 to 500 bil-\\nlion tokens are trained to get the estimates for compute-optimal\\ntraining under a given budget. The authors train a 70B model\\nwith the same compute budget as Gopher (280B) but with 4\\ntimes more data. It outperforms Gopher [116], GPT-3 [6], and\\nothers on various downstream tasks, after fine-tuning.\\nAlexaTM [122]: An encoder-decoder model, where encoder\\nweights and decoder embeddings are initialized with a pre-\\ntrained encoder to speed up training. The encoder stays frozen\\nfor the initial 100k steps and is later unfrozen for end-to-end\\ntraining. The model is trained on a combination of denoising\\nand causal language modeling (CLM) objectives, concatenat-\\ning a [CLM] token at the beginning for mode switching. Dur-\\ning training, the CLM task is applied for 20% of the time, which\\nimproves the in-context learning performance.\\nPaLM [15]: A causal decoder with parallel attention and\\nfeed-forward layers similar to Eq. 4, speeding up training by\\na factor of 15. Additional changes to the conventional trans-\\nformer model include SwiGLU activation, RoPE embeddings,\\nmulti-query attention that saves computation cost during decod-\\ning, and shared input-output embeddings. During training, loss\\nspiking was observed, and to fix it, model training was restarted\\nfrom a 100-step earlier checkpoint by skipping 200-500 batches\\naround the spike. Moreover, the model was found to memo-\\nrize around 2.4% of the training data at the 540B model scale,\\nwhereas this number was lower for smaller models.\\nPaLM-2 [123]: A smaller multi-lingual variant of PaLM,\\ntrained for larger iterations on a better quality dataset. PaLM-\\n2 shows significant improvements over PaLM, while reducing\\ntraining and inference costs due to its smaller size. To lessen\\ntoxicity and memorization, it appends special tokens with a\\nfraction of pre-training data, which shows a reduction in gener-\\nating harmful responses.\\nU-PaLM [124]: This method trains PaLM for 0.1% addi-\\ntional compute with the UL2 (also named as UL2Restore) ob-\\njective [125], using the same dataset it outperforms the baseline\\nsignificantly on various NLP tasks, including zero-shot, few-\\nshot, commonsense reasoning, CoT, etc. Training with UL2R\\ninvolves converting a causal decoder PaLM to a non-causal de-\\ncoder PaLM and employing 50% sequential denoising, 25%\\nregular denoising, and 25% extreme denoising loss functions.\\n9', 'UL2 [125]: An encoder-decoder architecture trained using a\\nmixture of denoisers (MoD) objective. Denoisers include 1)\\nR-Denoiser: a regular span masking, 2) S-Denoiser: which cor-\\nrupts consecutive tokens of a large sequence and 3) X-Denoiser:\\nwhich corrupts a large number of tokens randomly. During pre-\\ntraining, UL2 includes a denoiser token from R, S, X to rep-\\nresent a denoising setup. It helps improve fine-tuning perfor-\\nmance for downstream tasks that bind the task to one of the up-\\nstream training modes. This MoD style of training outperforms\\nthe T5 model on many benchmarks.\\nGLM-130B [33]: GLM-130B is a bilingual (English and Chi-\\nnese) model trained using an auto-regressive mask infilling pre-\\ntraining objective similar to the GLM [126]. This training style\\nmakes the model bidirectional as compared to GPT-3, which is\\nunidirectional. As opposed to GLM, the training of GLM-130B\\nincludes a small amount of multi-task instruction pre-training\\ndata (5% of the total data) along with self-supervised mask in-\\nfilling. To stabilize the training, it applies embedding layer gra-\\ndient shrink.\\nLLaMA [127, 21]: A set of decoder-only language models\\nvarying from 7B to 70B parameters. LLaMA models series is\\nthe most famous among the community for parameter efficiency\\nand instruction tuning.\\nLLaMA-1 [127]: Implements efficient causal attention [128]\\nby not storing and computing masked attention weights and\\nkey/query scores. Another optimization is reducing the number\\nof activations recomputed in the backward pass, as in [129].\\nLLaMA-2 [21]: This work is more focused on fine-tuning a\\nsafer and better LLaMA-2-Chat model for dialogue generation.\\nThe pre-trained model has 40% more training data with a larger\\ncontext length and grouped-query attention.\\nLLaMA-3/3.1 [130]: A collection of models trained on a\\nseven times larger dataset as compared to LLaMA-2 with dou-\\nble the context length, outperforming its previous variants and\\nother models.\\nPanGu-Σ [92]: An autoregressive model with parameters\\ncopied from PanGu-α and extended to a trillion scale with Ran-\\ndom Routed Experts (RRE), the architectural diagram is shown\\nin Figure 10. RRE is similar to the MoE architecture, with\\ndistinctions at the second level, where tokens are randomly\\nrouted to experts in a domain instead of using a learnable gat-\\ning method. The model has bottom layers densely activated and\\nshared across all domains, whereas top layers are sparsely ac-\\ntivated according to the domain. This training style allows for\\nextracting task-specific models and reduces catastrophic forget-\\nting effects in the case of continual learning.\\nMixtral8x22b [131]: A mixture-of-experts (MoE) model with\\neight distinct experts routes each token to two experts at each\\nlayer and combines the outputs additively.\\nSnowflake Arctic [132]: Arctic LLM is a hybrid of dense and\\nmixture-of-experts (MoE) architecture. The MoE (128×3.66B\\nMLP experts) is parallel to the dense transformer (10B) with\\nonly two experts activated. The model has many experts, com-\\npared to other MoE LLMs [131, 133], to increase the model\\ncapacity and provide an opportunity to choose among many ex-\\nperts for a diverse configuration. The model has 480B param-\\neters, and only 17B are active during a forward pass, reducing\\nthe computation significantly.\\nGrok [133, 134]: Grok is a family of LLMs including Grok-1\\nand Grok-1.5, released by XAI.\\nGrok-1 [133]: Grok-1 is a 314B parameters language MoE\\nmodel (eight experts), where two experts are activated per to-\\nken.\\nGrok-1.5 [134]: Grok-1.5 is a multi-modal LLM with a larger\\ncontext length and improved performance.\\nGemini [135, 136]: Gemini replaces Bard (based on PaLM)\\nwith multi-modal capabilities and significant language model-\\ning performance improvements.\\nGemini-1 [135]: The first-ever auto-regressive model to\\nachieve human-level capabilities on the MMLU benchmark.\\nGemini-1.5 [136]: A multi-modal LLM with MoE architec-\\nture builds on the findings of Gemini-1. The model has a 2M\\ncontext window and can reason over information up to 10M\\ntokens. Such large context windows were never achieved pre-\\nviously and shown to have a huge impact on performance gain.\\nNemotron-4 340B [137]: A decoder-only model that has been\\naligned on 98% synthetic data and only 2% manually annotated\\ndata. Utilizing synthetic data at a large proportion improves the\\nmodel performance significantly. The paper suggested intro-\\nducing alignment data with a smaller subset of previously seen\\ndata during the late stage of the model pre-training, enabling the\\nsmooth transition from the pre-trained stage to the final train-\\ning stage. To train better instruction-following models, weaker\\nmodels are trained into stronger models iteratively. The syn-\\nthetic data generated by the weaker instruction-tuned model is\\nused to train a base model which is later supervised fine-tuned\\noutperforming the weaker model.\\nDeepSeek [138]: DeepSeek studies the LLMs scaling laws\\nin detail to determine the optimal non-embedding model size\\nand training data. The experiments were performed for 8 bud-\\ngets ranging from 1e17 to 3e20 training FLOPs. Each compute\\nbudget was tested against ten different models/data scales. The\\nbatch size and learning rates were also fitted for the given com-\\npute budget finding that the batch size should increase with\\nthe increased compute budget while decreasing the learning\\nrate. Following are the equations for the optimal batch-size (B),\\nlearning rate (η), model size (M), and data (D):\\nBopt = 0.2920.C0.3271\\nηopt = 0.3118.C−0.1250\\nMopt = Mbase.Ca\\nDopt = Dbase.Cb\\nMbase = 0.1715, Dbase = 5.8316, a = 0.5243, b = 0.4757\\n(5)\\nDeepSeek-v2 [139]: An MoE model that introduces multi-\\nhead latent attention (MLA) to reduce inference costs, by com-\\npressing Key-Value (KV) cache into a latent vector.\\nMLA\\nachieves better performance than multi-head attention (MHA),\\nand other efficient attention mechanisms such as grouped query\\nattention (GQA), multi-query attention (MQA), etc. Because\\nof MLA, DeepSeek-v2 achieves 5.76 times faster inference\\nthroughput as compared to DeepSeek [138].\\n10']}}\n",
      "{'split_text': {'chunks': [Document(metadata={}, page_content='A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,\\nNick Barnesi, Ajmal Mianj\\naThe University of Sydney, Sydney, Australia\\nbUniversity of Engineering and Technology (UET), Lahore, Pakistan\\ncThe Chinese University of Hong Kong (CUHK), HKSAR, China\\ndUniversity of Technology Sydney (UTS), Sydney, Australia\\neCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\\nfKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\\ngSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\\nhThe University of Melbourne (UoM), Melbourne, Australia\\niAustralian National University (ANU), Canberra, Australia\\njThe University of Western Australia (UWA), Perth, Australia\\nAbstract\\nLarge Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\\nbeyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in\\nLLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\\nthe rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise\\nyet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on a broad\\nrange of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts\\nalong with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only a\\nsystematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive,\\ninformative summaries of the existing works to advance the LLM research.\\nKeywords:\\nLarge Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\\n1. Introduction\\nLanguage plays a fundamental role in facilitating commu-\\nnication and self-expression for humans and their interaction\\nwith machines. The need for generalized models stems from\\nthe growing demand for machines to handle complex language\\ntasks, including translation, summarization, information re-\\ntrieval, conversational interactions, etc. Recently, significant\\nbreakthroughs have been witnessed in language models, pri-\\nmarily attributed to transformers [1], increased computational\\ncapabilities, and the availability of large-scale training data.\\nThese developments have brought about a revolutionary trans-\\nformation by enabling the creation of LLMs that can approxi-\\nmate human-level performance on various tasks [2, 3]. Large\\n∗Equal contribution\\nEmail addresses: humza_naveed@yahoo.com (Humza Naveed),\\naukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi\\nQiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),\\nmuhammad.usman@kfupm.edu.sa (Muhammad Usman),\\nnaveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\\nnick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\\n(Ajmal Mian)\\nFigure 1: The trend of papers released over the years containing keywords\\n\"Large Language Model\", \"Large Language Model + Fine-Tuning\", and \"Large\\nLanguage Model + Alignment\".\\nPreprint submitted to Elsevier\\nOctober 18, 2024\\narXiv:2307.06435v10  [cs.CL]  17 Oct 2024'), Document(metadata={}, page_content='2019\\nT5 (Oct)\\nGPT-3 (May)\\nWebGPT (Dec)\\nOPT-IML\\nTK-Instruct (May)\\nmT0 (Dec)\\nWizard-LM\\nVicuna\\nAlpaca (Mar)\\nHuaTuo (Apr)\\nKoala (May)\\nWizard-Coder (Jun)\\nGoat\\nPanGu-α (Apr)\\nCPM-2 (Jun)\\nGPT-NeoX-20B (Apr)\\nCodeGen (Mar) \\nGalactica (Nov)\\nGLM (Oct)\\nOPT\\nUL2 (May)\\nLLaMA (Feb)\\nLLaMA 2 (Jul)\\nMPT (Jun)\\nCodeT5+\\nCode Llama (Aug)\\nStarCoder\\nXuan Yuan 2.0 (May)\\n2020\\n2021\\n2022\\n2023\\n2024\\nmT5 (Oct)\\nHyperCLOVA (Sep)\\nERNIE 3.0\\nCodex (Jul)\\nJurassic-1 (Aug)\\nYuan 1.0 (Oct)\\nGopher (Dec)\\nERNIE 3.0 Titan\\nGLaM\\nLaMDA\\nT0 (Oct)\\nChatGPT (Nov)\\nSparrow (Sep)\\nFLAN-U-PaLM (Oct)\\nBard (Oct)\\nMT-NLG (Jan)\\nAlphaCode (Feb)\\nChinchilla (Mar)\\nPaLM (Apr)\\nU-PALM (Oct)\\nBLOOM (Nov)\\nAlexaTM (Aug)\\nPaLM2 (May)\\nGPT-4\\nPanGu-Σ (Mar)\\nBloombergGPT\\nClaude\\nGemini (Dec)\\nDeepSeek (Jan)\\nLLaMA 3 \\nGrok-1 (Mar)\\nSnowflake Arctic (Apr)\\nDeepSeek-V2 (May)\\nMixtral 8x22B\\nNemotron (Feb)\\nGPT-4o (May)\\nOpenAI o1 (Sep)\\nGemini-1.5 (Feb)\\nGrok-1.5 (Apr)\\nFigure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models. Models\\non the upper half signify open-source availability, whereas those on the bottom are closed-source. The chart illustrates the increasing trend towards instruction-tuned\\nand open-source models, highlighting the evolving landscape and trends in natural language processing research.\\nLanguage Models (LLMs) have emerged as cutting-edge arti-\\nficial intelligence systems that can process and generate text\\nwith coherent communication [4] and generalize to multiple\\ntasks [5, 6].\\nThe historical progress in natural language processing (NLP)\\nevolved from statistical to neural language modeling and then\\nfrom pre-trained language models (PLMs) to LLMs.\\nWhile\\nconventional language modeling (LM) trains task-specific mod-\\nels in supervised settings, PLMs are trained in a self-supervised\\nsetting on a large corpus of text [7, 8, 9] with the aim of learning\\na generic representation that is shareable among various NLP\\ntasks. After fine-tuning for downstream tasks, PLMs surpass\\nthe performance gains of traditional language modeling (LM).\\nThe larger PLMs bring more performance gains, which has led\\nto the transitioning of PLMs to LLMs by significantly increas-\\ning model parameters (tens to hundreds of billions) [10] and\\ntraining dataset (many GBs and TBs) [10, 11]. Following this\\ndevelopment, numerous LLMs have been proposed in the lit-\\nerature [10, 11, 12, 6, 13, 14, 15]. An increasing trend in the\\nnumber of released LLMs and names of a few significant LLMs\\nproposed over the years are shown in Fig 1 and Fig 2, respec-\\ntively.\\nThe early work on LLMs, such as T5 [10] and mT5 [11] em-\\nployed transfer learning until GPT-3 [6] showed LLMs are\\nzero-shot transferable to downstream tasks without fine-tuning.\\nLLMs accurately respond to task queries when prompted with\\ntask descriptions and examples. However, pre-trained LLMs\\nfail to follow user intent and perform worse in zero-shot set-\\ntings than in few-shot.\\nFine-tuning them with task instruc-\\ntions data [16, 17, 18, 19] and aligning with human prefer-\\nences [20, 21] enhances generalization to unseen tasks, im-\\nproving zero-shot performance significantly and reducing mis-\\naligned behavior.\\nIn addition to better generalization and domain adaptation,\\nLLMs appear to have emergent abilities, such as reasoning,\\nplanning, decision-making, in-context learning, answering in\\nzero-shot settings, etc.\\nThese abilities are known to be ac-\\nquired by them due to their gigantic scale even when the pre-\\ntrained LLMs are not trained specifically to possess these at-\\ntributes [22, 23, 24]. Such abilities have led LLMs to be widely\\nadopted in diverse settings, including multi-modal, robotics,\\ntool manipulation, question answering, autonomous agents, etc.\\nVarious improvements have also been suggested in these areas\\neither by task-specific training [25, 26, 27, 28, 29, 30, 31] or\\nbetter prompting [32].\\nThe LLMs abilities to solve diverse tasks with human-level\\nperformance come at the cost of slow training and inference,\\nextensive hardware requirements, and higher running costs.\\nSuch requirements have limited their adoption and opened up\\nopportunities to devise better architectures [15, 33, 34, 35]\\nand training strategies [36, 37, 21, 38, 39, 40, 41].\\nParam-\\neter efficient tuning [38, 41, 40], pruning [42, 43], quantiza-\\ntion [44, 45], knowledge distillation, and context length inter-\\npolation [46, 47, 48, 49] among others are some of the methods\\nwidely studied for efficient LLM utilization.\\nDue to the success of LLMs on a wide variety of tasks, the\\nresearch literature has recently experienced a large influx of\\nLLM-related contributions.\\nResearchers have organized the\\nLLMs literature in surveys [50, 51, 52, 53], and topic-specific\\nsurveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our\\ncontribution focuses on providing a comprehensive yet concise\\noverview of the general direction of LLM research. This arti-\\ncle summarizes architectural and training details of pre-trained\\nLLMs and delves deeper into the details of concepts like fine-\\ntuning, multi-modal LLMs, augmented LLMs, datasets, eval-\\nuation, applications, challenges, and others to provide a self-\\ncontained comprehensive overview. Our key contributions are\\nsummarized as follows.\\n• We present a survey on the developments in LLM research,\\nproviding a concise, comprehensive overview of the direc-\\ntion.\\n• We present extensive summaries of pre-trained models that\\ninclude fine-grained details of architecture and training de-\\ntails.\\n• We summarize major findings of the popular contributions\\nand provide a detailed discussion on the key design and\\ndevelopment aspects of LLMs to help practitioners effec-\\ntively leverage this technology.\\n• In this self-contained article, we cover a range of con-\\ncepts to present the general direction of LLMs compre-\\nhensively, including background, pre-training, fine-tuning,\\n2'), Document(metadata={}, page_content='Figure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1. Pre-Training 2. Fine-Tuning 3. Efficient 4. Inference 5. Evaluation 6. Applications\\n7. Challenges\\nmulti-modal LLMs, augmented LLMs, LLMs-powered\\nagents, datasets, evaluation, etc.\\nWe loosely follow the existing terminology to ensure a stan-\\ndardized outlook of this research direction. For instance, fol-\\nlowing [50], our survey discusses pre-trained LLMs with 10B\\nparameters or more. We refer the readers interested in smaller\\npre-trained models to [51, 52, 53].\\nThe organization of this paper is as follows. Section 2 discusses\\nthe background of LLMs. Section 3 focuses on LLMs overview,\\narchitectures, training pipelines and strategies, fine-tuning, and\\nutilization in different domains. Section 4 highlights the config-\\nuration and parameters that play a crucial role in the function-\\ning of these models. Summary and discussions are presented\\nin section 3.8. The LLM training and evaluation, datasets, and\\nbenchmarks are discussed in section 5, followed by challenges\\nand future directions, and conclusion in sections 7 and 8, re-\\nspectively.\\n3'), Document(metadata={}, page_content='2. Background\\nWe provide the relevant background to understand the fun-\\ndamentals related to LLMs in this section. We briefly discuss\\nnecessary components in LLMs and refer the readers interested\\nin details to the original works.\\n2.1. Tokenization\\nTokenization [59] is an essential pre-processing step in\\nLLM training that parses the text into non-decomposing units\\ncalled tokens. Tokens can be characters, subwords [60], sym-\\nbols [61], or words, depending on the tokenization process.\\nSome of the commonly used tokenization schemes in LLMs\\ninclude wordpiece [62], byte pair encoding (BPE) [61], and un-\\nigramLM [60]. Readers are encouraged to refer to [63] for a\\ndetailed survey.\\n2.2. Encoding Positions\\nThe transformer processes input sequences in parallel and\\nindependently of each other.\\nMoreover, the attention mod-\\nule in the transformer does not capture positional information.\\nAs a result, positional encodings were introduced in trans-\\nformer [64], where a positional embedding vector is added to\\nthe token embedding. Variants of positional embedding include\\nabsolute, relative, or learned positional encodings. Within rel-\\native encoding, Alibi and RoPE are two widely used positional\\nembeddings in LLMs.\\nAlibi [65]: It subtracts a scalar bias from the attention score\\nthat increases with the distance between token positions. This\\nfavors using recent tokens for attention.\\nRoPE [66]: It rotates query and key representations at an an-\\ngle proportional to the token absolute position in the input\\nsequence, resulting in a relative positional encoding scheme\\nwhich decays with the distance between the tokens.\\n2.3. Attention in LLMs\\nAttention assigns weights to input tokens based on impor-\\ntance so that the model gives more emphasis to relevant tokens.\\nAttention in transformers [64] calculates query, key, and value\\nmappings for input sequences, where the attention score is\\nobtained by multiplying the query and key, and later used to\\nweight values. We discuss different attention strategies used in\\nLLMs below.\\nSelf-Attention [64]: Calculates attention using queries, keys,\\nand values from the same block (encoder or decoder).\\nCross Attention: It is used in encoder-decoder architectures,\\nwhere encoder outputs are the queries, and key-value pairs\\ncome from the decoder.\\nSparse Attention [67]: Self-attention has O(n2) time complex-\\nity which becomes infeasible for large sequences. To speed\\nup the computation, sparse attention [67] iteratively calculates\\nattention in sliding windows for speed gains.\\nFlash Attention [68]: Memory access is the major bottleneck\\nin calculating attention using GPUs.\\nTo speed up, flash\\nattention employs input tiling to minimize the memory reads\\nand writes between the GPU high bandwidth memory (HBM)\\nand the on-chip SRAM.\\n2.4. Activation Functions\\nThe activation functions serve a crucial role in the curve-\\nfitting abilities of neural networks [69]. We discuss activation\\nfunctions used in LLMs in this section.\\nReLU [70]: The Rectified linear unit (ReLU) is defined as:\\nReLU(x) = max(0, x)\\n(1)\\nGeLU [71]: The Gaussian Error Linear Unit (GeLU) is the\\ncombination of ReLU, dropout [72] and zoneout [73].\\nGLU variants [74]: The Gated Linear Unit [75] is a neural\\nnetwork layer that is an element-wise product (⊗) of a linear\\ntransformation and a sigmoid transformed (σ) linear projection\\nof the input given as:\\nGLU(x, W, V, b, c) = (xW + b) ⊗σ(xV + c),\\n(2)\\nwhere X is the input of layer and l, W, b, V and c are learned\\nparameters. Other GLU variants [74] used in LLMs are:\\nReGLU(x, W, V, b, c) = max(0, xW + b)⊗,\\nGEGLU(x, W, V, b, c) = GELU(xW + b) ⊗(xV + c),\\nS wiGLU(x, W, V, b, c, β) = S wishβ(xW + b) ⊗(xV + c).\\n2.5. Layer Normalization\\nLayer normalization leads to faster convergence and is an in-\\ntegrated component of transformers [64]. In addition to Layer-\\nNorm [76] and RMSNorm [77], LLMs use pre-layer normal-\\nization [78], applying it before multi-head attention (MHA).\\nPre-norm is shown to provide training stability in LLMs. An-\\nother normalization variant, DeepNorm [79] fixes the issue with\\nlarger gradients in pre-norm.\\n2.6. Distributed LLM Training\\nThis section describes distributed LLM training approaches\\nbriefly. More details are available in [13, 37, 80, 81].\\nData Parallelism: Data parallelism replicates the model on\\nmultiple devices where data in a batch gets divided across de-\\nvices. At the end of each training iteration weights are synchro-\\nnized across all devices.\\nTensor Parallelism: Tensor parallelism shards a tensor compu-\\ntation across devices. It is also known as horizontal parallelism\\nor intra-layer model parallelism.\\nPipeline Parallelism: Pipeline parallelism shards model layers\\nacross different devices. This is also known as vertical paral-\\nlelism.\\nModel Parallelism: A combination of tensor and pipeline par-\\nallelism is known as model parallelism.\\n3D Parallelism: A combination of data, tensor, and model par-\\nallelism is known as 3D parallelism.\\nOptimizer Parallelism: Optimizer parallelism also known as\\nzero redundancy optimizer [37] implements optimizer state\\npartitioning, gradient partitioning, and parameter partitioning\\nacross devices to reduce memory consumption while keeping\\nthe communication costs as low as possible.\\n4'), Document(metadata={}, page_content='2.7. Libraries\\nSome commonly used libraries for LLMs training are:\\nTransformers [82]: The library provides access to various pre-\\ntrained transformer models with APIs to train, fine-tune, infer,\\nand develop custom models.\\nDeepSpeed [36]: A library for scalable distributed training and\\ninference of deep learning models.\\nMegatron-LM [80]: It provides GPU-optimized techniques for\\nlarge-scale training of LLMs.\\nJAX [83]: A Python library for high-performance numerical\\ncomputing and scaleable machine learning. It can differenti-\\nate native Python and NumPy functions and execute them on\\nGPUs.\\nColossal-AI [84]: A collection of components to write dis-\\ntributed deep learning models.\\nBMTrain [81]: A library to write efficient stand-alone LLMs\\ntraining code.\\nFastMoE [85]:\\nProvides API to build mixture-of-experts\\n(MoE) model in PyTorch.\\nMindSpore [86]: A deep learning training and inference frame-\\nwork extendable to mobile, edge, and cloud computing.\\nPyTorch [87]: A framework developed by Facebook AI Re-\\nsearch lab (FAIR) to build deep learning models. The main\\nfeatures of PyTorch include a dynamic computation graph and\\na pythonic coding style.\\nTensorflow [88]:\\nA deep learning framework written by\\nGoogle. The key features of TensorFlow are graph-based com-\\nputation, eager execution, scalability, etc.\\nMXNet [89]: Apache MXNet is a deep learning framework\\nwith support to write programs in multiple languages, includ-\\ning, Python, C++, Scala, R, etc. It also provides support for\\ndynamic and static computation graphs.\\n2.8. Data PreProcessing\\nThis section briefly summarizes data preprocessing tech-\\nniques used in LLMs training.\\nQuality Filtering: For better results, training data quality is\\nessential. Some approaches to filtering data are: 1) classifier-\\nbased and 2) heuristics-based.\\nClassifier-based approaches\\ntrain a classifier on high-quality data and predict the quality of\\ntext for filtering, whereas heuristics-based employ some rules\\nfor filtering like language, metrics, statistics, and keywords.\\nData Deduplication: Duplicated data can affect model per-\\nformance and increase data memorization; therefore, to train\\nLLMs, data deduplication is one of the preprocessing steps.\\nThis can be performed at multiple levels, like sentences,\\ndocuments, and datasets.\\nPrivacy Reduction: Most of the training data for LLMs is\\ncollected through web sources.\\nThis data contains private\\ninformation; therefore, many LLMs employ heuristics-based\\nmethods to filter information such as names, addresses, and\\nphone numbers to avoid learning personal information.\\n2.9. Architectures\\nHere we discuss the variants of the transformer architectures\\nused in LLMs. The difference arises due to the application of\\nFigure 4: An example of attention patterns in language models, image is taken\\nfrom [93].\\nFigure 5: An example of language model training objectives, image from [93].\\nthe attention and the connection of transformer blocks. An il-\\nlustration of attention patterns of these architectures is shown\\nin Figure 4.\\nEncoder Decoder: This architecture processes inputs through\\nthe encoder and passes the intermediate representation to the\\ndecoder to generate the output.\\nHere, the encoder sees the\\ncomplete sequence utilizing self-attention whereas the decoder\\nprocesses the sequence one after the other with implementing\\ncross-attention.\\nCausal Decoder: A type of architecture that does not have an\\nencoder and processes and generates output using a decoder,\\nwhere the predicted token depends only on the previous time\\nsteps.\\nPrefix Decoder: It is also known as a non-causal decoder,\\nwhere the attention calculation is not strictly dependent on the\\npast information and the attention is bidirectional. An example\\nof a non-causal attention mask is shown in Figure 4.\\nMixture-of-Experts: It is a variant of transformer architecture\\nwith parallel independent experts and a router to route tokens\\nto experts. These experts are feed-forward layers after the at-\\ntention block [90]. Mixture-of-Experts (MoE) is an efficient\\nsparse architecture that offers comparable performance to dense\\nmodels and allows increasing the model size without increas-\\ning the computational cost by activating only a few experts at a\\ntime [91, 92].\\n2.10. Pre-Training Objectives\\nThis section describes LLMs pre-training objectives.\\nFor\\nmore details see the paper [93].\\nFull Language Modeling: An autoregressive language model-\\ning objective where the model is asked to predict future tokens\\ngiven the previous tokens, an example is shown in Figure 5.\\nPrefix Language Modeling: A non-causal training objective,\\nwhere a prefix is chosen randomly and only remaining target\\ntokens are used to calculate the loss. An example is shown in\\nFigure 5.\\n5'), Document(metadata={}, page_content='Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting/utilization. Prompting LLMs to generate responses is possible at\\ndifferent training stages like pre-training, instruction-tuning, or alignment tuning. “RL” stands for reinforcement learning, “RM” represents reward-modeling, and\\n“RLHF” represents reinforcement learning with human feedback.\\nMasked Language Modeling: In this training objective, tokens\\nor spans (a sequence of tokens) are masked randomly and the\\nmodel is asked to predict masked tokens given the past and\\nfuture context. An example is shown in Figure 5.\\nUnified Language Modeling: Unified language modeling [94]\\nis a combination of causal, non-causal, and masked language\\ntraining objectives. Here in masked language modeling, the\\nattention is not bidirectional but unidirectional, attending either\\nleft-to-right or right-to-left context.\\n2.11. LLMs Scaling Laws\\nScaling laws study the optimal combination of model param-\\neters, dataset size, and computational resources that predict the\\nimprovement in the model performance. It has been shown\\nthat the loss scales according to the power-law with model size,\\ndataset size, and compute resources [95]. This study suggests\\nlarger models are more important than big data for better perfor-\\nmance. Another variant of scaling law [96] suggests the model\\nsize and the number of training tokens should be scaled equally.\\n2.12. LLMs Adaptation Stages\\nThis section discusses the fundamentals of LLMs adaptation\\nstages, from pre-training to fine-tuning for downstream tasks\\nand utilization. An example of different training stages and in-\\nference in LLMs is shown in Figure 6. In this paper, we refer\\nto alignment-tuning as aligning with human preferences, while\\noccasionally the literature uses the term alignment for different\\npurposes.\\n2.12.1. Pre-Training\\nIn the very first stage, the model is trained in a self-\\nsupervised manner on a large corpus to predict the next to-\\nkens given the input. The design choices of LLMs vary from\\nencoder-decoder to decoder-only architectures with different\\nbuilding blocks and loss functions in sections 2.5, 2.4, 2.10.\\n2.12.2. Fine-Tuning\\nThere are different styles to fine-tune an LLM. This section\\nbriefly discusses fine-tuning approaches.\\nTransfer Learning: The pre-trained LLMs perform well for\\nvarious tasks [6, 15]. However, to improve the performance for\\n6'), Document(metadata={}, page_content='a downstream task, pre-trained models are fine-tuned with the\\ntask-specific data [10, 11], known as transfer learning.\\nInstruction-tuning: To enable a model to respond to user\\nqueries effectively, the pre-trained model is fine-tuned on in-\\nstruction formatted data i.e., instruction and an input-output\\npair. Instructions generally comprise multi-task data in plain\\nnatural language, guiding the model to respond according to the\\nprompt and the input. This type of fine-tuning improves zero-\\nshot generalization and downstream task performance. Details\\non formatting instruction data and its various styles are avail-\\nable in [16, 50, 97].\\nAlignment-tuning: LLMs are prone to generating false, biased,\\nand harmful text. To make them helpful, honest, and harmless,\\nmodels are aligned using human feedback. Alignment involves\\nasking LLMs to generate unexpected responses and then updat-\\ning their parameters to avoid such responses [20, 21, 98].\\nIt ensures LLMs operate according to human intentions and\\nvalues. A model is defined to be an “aligned” model if the\\nmodel fulfills three criteria of helpful, honest, and harmless or\\n“HHH” [99].\\nResearchers employ reinforcement learning with human feed-\\nback (RLHF) [100] for model alignment. In RLHF, a fine-tuned\\nmodel on demonstrations is further trained with reward model-\\ning (RM) and reinforcement learning (RL), shown in Figure 6.\\nBelow we briefly discuss RM and RL pipelines in RLHF.\\nReward modeling: trains a model to rank generated responses\\naccording to human preferences using a classification objec-\\ntive. To train the classifier humans annotate LLMs generated\\nresponses based on the HHH criteria.\\nReinforcement learning: in combination with the reward model\\nis used for alignment in the next stage. The previously trained\\nreward model ranks LLM-generated responses into preferred\\nvs. non-preferred, which is used to align the model with proxi-\\nmal policy optimization (PPO). This process repeats iteratively\\nuntil convergence.\\n2.12.3. Prompting/Utilization\\nPrompting is a method to query trained LLMs for generating\\nresponses, as illustrated in Figure 6. LLMs can be prompted in\\nvarious prompt setups, where they can be adapted to the instruc-\\ntions without fine-tuning and in other cases with fine-tuning on\\ndata containing different prompt styles [16, 101, 102]. A good\\nguide on prompt engineering is available at [32]. Below, we\\nwill discuss various widely used prompt setups.\\nZero-Shot Prompting: LLMs are zero-shot learners and ca-\\npable of answering queries never seen before. This style of\\nprompting requires LLMs to answer user questions without see-\\ning any examples in the prompt.\\nIn-context Learning: Also known as few-shot learning, here,\\nmultiple input-output demonstration pairs are shown to the\\nmodel to generate the desired response. This adaptation style\\nis also called few-shot learning. A discussion on formatting in-\\ncontext learning (ICL) templates is available in [54, 50, 18, 16].\\nReasoning in LLMs: LLMs are zero-shot reasoners and can\\nbe provoked to generate answers to logical problems, task\\nplanning, critical thinking, etc. with reasoning.\\nGenerating\\nreasons is possible only by using different prompting styles,\\nwhereas to improve LLMs further on reasoning tasks many\\nmethods [16, 97] train them on reasoning datasets. We discuss\\nvarious prompting techniques for reasoning below.\\nChain-of-Thought (CoT): A special case of prompting where\\ndemonstrations contain reasoning information aggregated with\\ninputs and outputs so that the model generates outcomes with\\nstep-by-step reasoning. More details on CoT prompts are avail-\\nable in [55, 103, 101].\\nSelf-Consistency:\\nImproves CoT performance by generat-\\ning multiple responses and selecting the most frequent an-\\nswer [104].\\nTree-of-Thought (ToT): Explores multiple reasoning paths\\nwith possibilities to look ahead and backtrack for problem-\\nsolving [105].\\nSingle-Turn Instructions: In this prompting setup, LLMs are\\nqueried only once with all the relevant information in the\\nprompt. LLMs generate responses by understanding the con-\\ntext either in a zero-shot or few-shot setting.\\nMulti-Turn Instructions: Solving a complex task requires mul-\\ntiple interactions with LLMs, where feedback and responses\\nfrom the other tools are given as input to the LLM for the next\\nrounds. This style of using LLMs in the loop is common in\\nautonomous agents.\\n3. Large Language Models\\nThis section reviews LLMs, briefly describing their architec-\\ntures, training objectives, pipelines, datasets, and fine-tuning\\ndetails.\\n3.1. Pre-Trained LLMs\\nHere, we provide summaries of various well-known pre-\\ntrained LLMs with significant discoveries, changing the course\\nof research and development in NLP. These LLMs have consid-\\nerably improved the performance in NLU and NLG domains,\\nand are widely fine-tuned for downstream tasks. Moreover, We\\nalso identify key findings and insights of pre-trained LLMs in\\nTable 1 and 2 that improve their performance.\\n3.1.1. General Purpose\\nT5 [10]: An encoder-decoder model employing a unified text-\\nto-text training for all NLP problems is shown in Figure 7. T5\\nplaces layer normalization outside the residual path in a conven-\\ntional transformer model [64]. It uses masked language mod-\\neling as a pre-training objective where spans (consecutive to-\\nkens) are replaced with a single mask instead of separate masks\\nfor each token. This type of masking speeds up the training as\\nit produces shorter sequences. After pre-training, the model is\\nfine-tuned using adapter layers [106] for downstream tasks.\\nGPT-3 [6]: The GPT-3 architecture is the same as the GPT-\\n2 [5] but with dense and sparse attention in transformer layers\\nsimilar to the Sparse Transformer [67]. It shows that large mod-\\nels can train on larger batch sizes with a lower learning rate to\\ndecide the batch size during training, GPT-3 uses the gradient\\nnoise scale as in [107]. Overall, GPT-3 increases model param-\\neters to 175B showing that the performance of large language\\n7'), Document(metadata={}, page_content='Figure 7: Unified text-to-text training example, source image from [10].\\nFigure 8: The image is the article of [108], showing an example of PanGu-α\\narchitecture.\\nmodels improves with the scale and is competitive with the fine-\\ntuned models.\\nmT5 [11]: A multilingual T5 model [10] trained on the mC4\\ndataset with 101 languages. The dataset is extracted from the\\npublic common crawl scrape. The model uses a larger vocab-\\nulary size of 250,000 to cover multiple languages. To avoid\\nover-fitting or under-fitting for a language, mT5 employs a data\\nsampling procedure to select samples from all languages. The\\npaper suggests using a small amount of pre-training datasets,\\nincluding all languages when fine-tuning for a task using En-\\nglish language data. This allows the model to generate correct\\nnon-English outputs.\\nPanGu-α [108]: An autoregressive model that has a query\\nlayer at the end of standard transformer layers, example shown\\nin Figure 8, to predict the next token. Its structure is similar to\\nthe transformer layer but with an additional embedding for the\\nnext position in the attention mechanism, given in Eq. 3.\\na = pnWq\\nhWk\\nhTHT\\nL\\n(3)\\nCPM-2 [12]: Cost-efficient Pre-trained language Models\\n(CPM-2) pre-trains bilingual (English and Chinese) 11B and\\n198B mixture-of-experts (MoE) models on the WuDaoCor-\\npus [109] dataset. The tokenization process removes “_” white\\nspace tokens in the sentencepiece tokenizer. The models are\\ntrained with knowledge inheritance, starting with only the Chi-\\nnese language in the first stage and then adding English and\\nChinese data. This trained model gets duplicated multiple times\\nto initialize the 198B MoE model. Moreover, to use the model\\nfor downstream tasks, CPM-2 experimented with both com-\\nplete fine-tuning and prompt fine-tuning as in [40] where only\\nprompt-related parameters are updated by inserting prompts at\\nvarious positions, front, middle, and back. CPM-2 also pro-\\nposes the INFMOE, a memory-efficient framework with a strat-\\negy to dynamically offload parameters to the CPU for inference\\nat a 100B scale. It overlaps data movement with inference com-\\nputation for lower inference time.\\nERNIE 3.0 [110]: ERNIE 3.0 takes inspiration from multi-\\ntask learning to build a modular architecture using Transformer-\\nXL [111] as the backbone. The universal representation mod-\\nule is shared by all the tasks, which serve as the basic block\\nfor task-specific representation modules, which are all trained\\njointly for natural language understanding, natural language\\ngeneration, and knowledge extraction. This LLM is primar-\\nily focused on the Chinese language. It claims to train on the\\nlargest Chinese text corpora for LLM training, and achieved\\nstate-of-the-art in 54 Chinese NLP tasks.\\nJurassic-1 [112]: A pair of auto-regressive language mod-\\nels, including a 7B-parameter J1-Large model and a 178B-\\nparameter J1-Jumbo model.\\nThe training vocabulary of\\nJurassic-1 comprise word pieces, complete words, and multi-\\nword expressions without any word boundaries, where possible\\nout-of-vocabulary instances are interpreted as Unicode bytes.\\nCompared to the GPT-3 counterparts, the Jurassic-1 models\\napply a more balanced depth-to-width self-attention architec-\\nture [113] and an improved tokenizer for a faster prediction\\nbased on broader resources, achieving a comparable perfor-\\nmance in zero-shot learning tasks and a superior performance in\\nfew-shot learning tasks given the ability to feed more examples\\nas a prompt.\\nHyperCLOVA [114]: A Korean language model with GPT-3\\narchitecture.\\nYuan 1.0 [115]: Trained on a Chinese corpus with 5TB of\\nhigh-quality text collected from the Internet. A Massive Data\\nFiltering System (MDFS) built on Spark is developed to pro-\\ncess the raw data via coarse and fine filtering techniques. To\\nspeed up the training of Yuan 1.0 to save energy expenses and\\ncarbon emissions, various factors that improve the performance\\nof distributed training are incorporated in architecture and train-\\ning: like increasing the hidden state size improves pipeline and\\ntensor parallelism performance, larger micro batches improve\\npipeline parallelism performance, and larger global batch size\\nimprove data parallelism performance. In practice, the Yuan 1.0\\nmodel performs well on text classification, Winograd Schema,\\nnatural language inference, and reading comprehension tasks.\\nGopher [116]: The Gopher family of models ranges from\\n44M to 280B parameters in size to study the effect of scale\\non the LLMs performance. The 280B model beats GPT-3 [6],\\nJurrasic-1 [112], MT-NLG [117], and others on 81% of the\\nevaluated tasks.\\nERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan extends ERNIE 3.0\\nby training a larger model with 26x the number of parameters\\nof the latter. This bigger model outperformed other state-of-the-\\nart models in 68 NLP tasks. LLMs produce text with incorrect\\nfacts. In order to have control of the generated text with fac-\\ntual consistency, ERNIE 3.0 Titan adds another task, Credible\\nand Controllable Generations, to its multi-task learning setup.\\n8'), Document(metadata={}, page_content='It introduces additional self-supervised adversarial and control-\\nlable language modeling losses to the pre-training step, which\\nenables ERNIE 3.0 Titan to beat other LLMs in their manually\\nselected Factual QA task set evaluations.\\nGPT-NeoX-20B [118]: An auto-regressive model that largely\\nfollows GPT-3 with a few deviations in architecture design,\\ntrained on the Pile dataset without any data deduplication. GPT-\\nNeoX has parallel attention and feed-forward layers in a trans-\\nformer block, given in Eq. 4, that increases throughput by 15%.\\nIt uses rotary positional embedding [66], applying it to only\\n25% of embedding vector dimension as in [119]. This reduces\\nthe computation without performance degradation. As opposed\\nto GPT-3, which uses dense and sparse layers, GPT-NeoX-20B\\nuses only dense layers. The hyperparameter tuning at this scale\\nis difficult; therefore, the model chooses hyperparameters from\\nthe method [6] and interpolates values between 13B and 175B\\nmodels for the 20B model. The model training is distributed\\namong GPUs using both tensor and pipeline parallelism.\\nx + Attn(LN1(x)) + FF(LN2(x))\\n(4)\\nOPT [14]: It is a clone of GPT-3, developed to open-source\\na model that replicates GPT-3 performance. Training of OPT\\nemploys dynamic loss scaling [120] and restarts from an earlier\\ncheckpoint with a lower learning rate whenever loss divergence\\nis observed. Overall, the performance of OPT-175B models is\\ncomparable to the GPT3-175B model.\\nBLOOM [13]: A causal decoder model trained on the ROOTS\\ncorpus to open-source an LLM. The architecture of BLOOM is\\nshown in Figure 9, with differences like ALiBi positional em-\\nbedding, an additional normalization layer after the embedding\\nlayer as suggested by the bitsandbytes1 library. These changes\\nstabilize training with improved downstream performance.\\nGLaM [91]: Generalist Language Model (GLaM) represents a\\nfamily of language models using a sparsely activated decoder-\\nonly mixture-of-experts (MoE) structure [121, 90].\\nTo gain\\nmore model capacity while reducing computation, the experts\\nare sparsely activated where only the best two experts are used\\nto process each input token. The largest GLaM model, GLaM\\n(64B/64E), is about 7× larger than GPT-3 [6], while only part of\\nthe parameters are activated per input token. The largest GLaM\\n(64B/64E) model achieves better overall results as compared\\nto GPT-3 while consuming only one-third of GPT-3’s training\\nenergy.\\nMT-NLG [117]: A 530B causal decoder based on the GPT-\\n2 architecture that has roughly 3× GPT-3 model parameters.\\nMT-NLG is trained on filtered high-quality data collected from\\nvarious public datasets and blends various types of datasets in a\\nsingle batch, which beats GPT-3 on several evaluations.\\nChinchilla [96]: A causal decoder trained on the same dataset\\nas the Gopher [116] but with a little different data sampling\\ndistribution (sampled from MassiveText). The model architec-\\nture is similar to the one used for Gopher, with the exception of\\nAdamW optimizer instead of Adam. Chinchilla identifies the\\n1https://github.com/TimDettmers/bitsandbytes\\nFigure 9: The BLOOM architecture example sourced from [13].\\nrelationship that model size should be doubled for every dou-\\nbling of training tokens. Over 400 language models ranging\\nfrom 70 million to over 16 billion parameters on 5 to 500 bil-\\nlion tokens are trained to get the estimates for compute-optimal\\ntraining under a given budget. The authors train a 70B model\\nwith the same compute budget as Gopher (280B) but with 4\\ntimes more data. It outperforms Gopher [116], GPT-3 [6], and\\nothers on various downstream tasks, after fine-tuning.\\nAlexaTM [122]: An encoder-decoder model, where encoder\\nweights and decoder embeddings are initialized with a pre-\\ntrained encoder to speed up training. The encoder stays frozen\\nfor the initial 100k steps and is later unfrozen for end-to-end\\ntraining. The model is trained on a combination of denoising\\nand causal language modeling (CLM) objectives, concatenat-\\ning a [CLM] token at the beginning for mode switching. Dur-\\ning training, the CLM task is applied for 20% of the time, which\\nimproves the in-context learning performance.\\nPaLM [15]: A causal decoder with parallel attention and\\nfeed-forward layers similar to Eq. 4, speeding up training by\\na factor of 15. Additional changes to the conventional trans-\\nformer model include SwiGLU activation, RoPE embeddings,\\nmulti-query attention that saves computation cost during decod-\\ning, and shared input-output embeddings. During training, loss\\nspiking was observed, and to fix it, model training was restarted\\nfrom a 100-step earlier checkpoint by skipping 200-500 batches\\naround the spike. Moreover, the model was found to memo-\\nrize around 2.4% of the training data at the 540B model scale,\\nwhereas this number was lower for smaller models.\\nPaLM-2 [123]: A smaller multi-lingual variant of PaLM,\\ntrained for larger iterations on a better quality dataset. PaLM-\\n2 shows significant improvements over PaLM, while reducing\\ntraining and inference costs due to its smaller size. To lessen\\ntoxicity and memorization, it appends special tokens with a\\nfraction of pre-training data, which shows a reduction in gener-\\nating harmful responses.\\nU-PaLM [124]: This method trains PaLM for 0.1% addi-\\ntional compute with the UL2 (also named as UL2Restore) ob-\\njective [125], using the same dataset it outperforms the baseline\\nsignificantly on various NLP tasks, including zero-shot, few-\\nshot, commonsense reasoning, CoT, etc. Training with UL2R\\ninvolves converting a causal decoder PaLM to a non-causal de-\\ncoder PaLM and employing 50% sequential denoising, 25%\\nregular denoising, and 25% extreme denoising loss functions.\\n9'), Document(metadata={}, page_content='UL2 [125]: An encoder-decoder architecture trained using a\\nmixture of denoisers (MoD) objective. Denoisers include 1)\\nR-Denoiser: a regular span masking, 2) S-Denoiser: which cor-\\nrupts consecutive tokens of a large sequence and 3) X-Denoiser:\\nwhich corrupts a large number of tokens randomly. During pre-\\ntraining, UL2 includes a denoiser token from R, S, X to rep-\\nresent a denoising setup. It helps improve fine-tuning perfor-\\nmance for downstream tasks that bind the task to one of the up-\\nstream training modes. This MoD style of training outperforms\\nthe T5 model on many benchmarks.\\nGLM-130B [33]: GLM-130B is a bilingual (English and Chi-\\nnese) model trained using an auto-regressive mask infilling pre-\\ntraining objective similar to the GLM [126]. This training style\\nmakes the model bidirectional as compared to GPT-3, which is\\nunidirectional. As opposed to GLM, the training of GLM-130B\\nincludes a small amount of multi-task instruction pre-training\\ndata (5% of the total data) along with self-supervised mask in-\\nfilling. To stabilize the training, it applies embedding layer gra-\\ndient shrink.\\nLLaMA [127, 21]: A set of decoder-only language models\\nvarying from 7B to 70B parameters. LLaMA models series is\\nthe most famous among the community for parameter efficiency\\nand instruction tuning.\\nLLaMA-1 [127]: Implements efficient causal attention [128]\\nby not storing and computing masked attention weights and\\nkey/query scores. Another optimization is reducing the number\\nof activations recomputed in the backward pass, as in [129].\\nLLaMA-2 [21]: This work is more focused on fine-tuning a\\nsafer and better LLaMA-2-Chat model for dialogue generation.\\nThe pre-trained model has 40% more training data with a larger\\ncontext length and grouped-query attention.\\nLLaMA-3/3.1 [130]: A collection of models trained on a\\nseven times larger dataset as compared to LLaMA-2 with dou-\\nble the context length, outperforming its previous variants and\\nother models.\\nPanGu-Σ [92]: An autoregressive model with parameters\\ncopied from PanGu-α and extended to a trillion scale with Ran-\\ndom Routed Experts (RRE), the architectural diagram is shown\\nin Figure 10. RRE is similar to the MoE architecture, with\\ndistinctions at the second level, where tokens are randomly\\nrouted to experts in a domain instead of using a learnable gat-\\ning method. The model has bottom layers densely activated and\\nshared across all domains, whereas top layers are sparsely ac-\\ntivated according to the domain. This training style allows for\\nextracting task-specific models and reduces catastrophic forget-\\nting effects in the case of continual learning.\\nMixtral8x22b [131]: A mixture-of-experts (MoE) model with\\neight distinct experts routes each token to two experts at each\\nlayer and combines the outputs additively.\\nSnowflake Arctic [132]: Arctic LLM is a hybrid of dense and\\nmixture-of-experts (MoE) architecture. The MoE (128×3.66B\\nMLP experts) is parallel to the dense transformer (10B) with\\nonly two experts activated. The model has many experts, com-\\npared to other MoE LLMs [131, 133], to increase the model\\ncapacity and provide an opportunity to choose among many ex-\\nperts for a diverse configuration. The model has 480B param-\\neters, and only 17B are active during a forward pass, reducing\\nthe computation significantly.\\nGrok [133, 134]: Grok is a family of LLMs including Grok-1\\nand Grok-1.5, released by XAI.\\nGrok-1 [133]: Grok-1 is a 314B parameters language MoE\\nmodel (eight experts), where two experts are activated per to-\\nken.\\nGrok-1.5 [134]: Grok-1.5 is a multi-modal LLM with a larger\\ncontext length and improved performance.\\nGemini [135, 136]: Gemini replaces Bard (based on PaLM)\\nwith multi-modal capabilities and significant language model-\\ning performance improvements.\\nGemini-1 [135]: The first-ever auto-regressive model to\\nachieve human-level capabilities on the MMLU benchmark.\\nGemini-1.5 [136]: A multi-modal LLM with MoE architec-\\nture builds on the findings of Gemini-1. The model has a 2M\\ncontext window and can reason over information up to 10M\\ntokens. Such large context windows were never achieved pre-\\nviously and shown to have a huge impact on performance gain.\\nNemotron-4 340B [137]: A decoder-only model that has been\\naligned on 98% synthetic data and only 2% manually annotated\\ndata. Utilizing synthetic data at a large proportion improves the\\nmodel performance significantly. The paper suggested intro-\\nducing alignment data with a smaller subset of previously seen\\ndata during the late stage of the model pre-training, enabling the\\nsmooth transition from the pre-trained stage to the final train-\\ning stage. To train better instruction-following models, weaker\\nmodels are trained into stronger models iteratively. The syn-\\nthetic data generated by the weaker instruction-tuned model is\\nused to train a base model which is later supervised fine-tuned\\noutperforming the weaker model.\\nDeepSeek [138]: DeepSeek studies the LLMs scaling laws\\nin detail to determine the optimal non-embedding model size\\nand training data. The experiments were performed for 8 bud-\\ngets ranging from 1e17 to 3e20 training FLOPs. Each compute\\nbudget was tested against ten different models/data scales. The\\nbatch size and learning rates were also fitted for the given com-\\npute budget finding that the batch size should increase with\\nthe increased compute budget while decreasing the learning\\nrate. Following are the equations for the optimal batch-size (B),\\nlearning rate (η), model size (M), and data (D):\\nBopt = 0.2920.C0.3271\\nηopt = 0.3118.C−0.1250\\nMopt = Mbase.Ca\\nDopt = Dbase.Cb\\nMbase = 0.1715, Dbase = 5.8316, a = 0.5243, b = 0.4757\\n(5)\\nDeepSeek-v2 [139]: An MoE model that introduces multi-\\nhead latent attention (MLA) to reduce inference costs, by com-\\npressing Key-Value (KV) cache into a latent vector.\\nMLA\\nachieves better performance than multi-head attention (MHA),\\nand other efficient attention mechanisms such as grouped query\\nattention (GQA), multi-query attention (MQA), etc. Because\\nof MLA, DeepSeek-v2 achieves 5.76 times faster inference\\nthroughput as compared to DeepSeek [138].\\n10'), Document(metadata={}, page_content='A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,\\nNick Barnesi, Ajmal Mianj\\naThe University of Sydney, Sydney, Australia\\nbUniversity of Engineering and Technology (UET), Lahore, Pakistan\\ncThe Chinese University of Hong Kong (CUHK), HKSAR, China\\ndUniversity of Technology Sydney (UTS), Sydney, Australia\\neCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\\nfKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\\ngSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\\nhThe University of Melbourne (UoM), Melbourne, Australia\\niAustralian National University (ANU), Canberra, Australia\\njThe University of Western Australia (UWA), Perth, Australia\\nAbstract\\nLarge Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\\nbeyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in\\nLLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\\nthe rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise\\nyet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on a broad\\nrange of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts\\nalong with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only a\\nsystematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive,\\ninformative summaries of the existing works to advance the LLM research.\\nKeywords:\\nLarge Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\\n1. Introduction\\nLanguage plays a fundamental role in facilitating commu-\\nnication and self-expression for humans and their interaction\\nwith machines. The need for generalized models stems from\\nthe growing demand for machines to handle complex language\\ntasks, including translation, summarization, information re-\\ntrieval, conversational interactions, etc. Recently, significant\\nbreakthroughs have been witnessed in language models, pri-\\nmarily attributed to transformers [1], increased computational\\ncapabilities, and the availability of large-scale training data.\\nThese developments have brought about a revolutionary trans-\\nformation by enabling the creation of LLMs that can approxi-\\nmate human-level performance on various tasks [2, 3]. Large\\n∗Equal contribution\\nEmail addresses: humza_naveed@yahoo.com (Humza Naveed),\\naukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi\\nQiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),\\nmuhammad.usman@kfupm.edu.sa (Muhammad Usman),\\nnaveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\\nnick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\\n(Ajmal Mian)\\nFigure 1: The trend of papers released over the years containing keywords\\n\"Large Language Model\", \"Large Language Model + Fine-Tuning\", and \"Large\\nLanguage Model + Alignment\".\\nPreprint submitted to Elsevier\\nOctober 18, 2024\\narXiv:2307.06435v10  [cs.CL]  17 Oct 2024'), Document(metadata={}, page_content='2019\\nT5 (Oct)\\nGPT-3 (May)\\nWebGPT (Dec)\\nOPT-IML\\nTK-Instruct (May)\\nmT0 (Dec)\\nWizard-LM\\nVicuna\\nAlpaca (Mar)\\nHuaTuo (Apr)\\nKoala (May)\\nWizard-Coder (Jun)\\nGoat\\nPanGu-α (Apr)\\nCPM-2 (Jun)\\nGPT-NeoX-20B (Apr)\\nCodeGen (Mar) \\nGalactica (Nov)\\nGLM (Oct)\\nOPT\\nUL2 (May)\\nLLaMA (Feb)\\nLLaMA 2 (Jul)\\nMPT (Jun)\\nCodeT5+\\nCode Llama (Aug)\\nStarCoder\\nXuan Yuan 2.0 (May)\\n2020\\n2021\\n2022\\n2023\\n2024\\nmT5 (Oct)\\nHyperCLOVA (Sep)\\nERNIE 3.0\\nCodex (Jul)\\nJurassic-1 (Aug)\\nYuan 1.0 (Oct)\\nGopher (Dec)\\nERNIE 3.0 Titan\\nGLaM\\nLaMDA\\nT0 (Oct)\\nChatGPT (Nov)\\nSparrow (Sep)\\nFLAN-U-PaLM (Oct)\\nBard (Oct)\\nMT-NLG (Jan)\\nAlphaCode (Feb)\\nChinchilla (Mar)\\nPaLM (Apr)\\nU-PALM (Oct)\\nBLOOM (Nov)\\nAlexaTM (Aug)\\nPaLM2 (May)\\nGPT-4\\nPanGu-Σ (Mar)\\nBloombergGPT\\nClaude\\nGemini (Dec)\\nDeepSeek (Jan)\\nLLaMA 3 \\nGrok-1 (Mar)\\nSnowflake Arctic (Apr)\\nDeepSeek-V2 (May)\\nMixtral 8x22B\\nNemotron (Feb)\\nGPT-4o (May)\\nOpenAI o1 (Sep)\\nGemini-1.5 (Feb)\\nGrok-1.5 (Apr)\\nFigure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models. Models\\non the upper half signify open-source availability, whereas those on the bottom are closed-source. The chart illustrates the increasing trend towards instruction-tuned\\nand open-source models, highlighting the evolving landscape and trends in natural language processing research.\\nLanguage Models (LLMs) have emerged as cutting-edge arti-\\nficial intelligence systems that can process and generate text\\nwith coherent communication [4] and generalize to multiple\\ntasks [5, 6].\\nThe historical progress in natural language processing (NLP)\\nevolved from statistical to neural language modeling and then\\nfrom pre-trained language models (PLMs) to LLMs.\\nWhile\\nconventional language modeling (LM) trains task-specific mod-\\nels in supervised settings, PLMs are trained in a self-supervised\\nsetting on a large corpus of text [7, 8, 9] with the aim of learning\\na generic representation that is shareable among various NLP\\ntasks. After fine-tuning for downstream tasks, PLMs surpass\\nthe performance gains of traditional language modeling (LM).\\nThe larger PLMs bring more performance gains, which has led\\nto the transitioning of PLMs to LLMs by significantly increas-\\ning model parameters (tens to hundreds of billions) [10] and\\ntraining dataset (many GBs and TBs) [10, 11]. Following this\\ndevelopment, numerous LLMs have been proposed in the lit-\\nerature [10, 11, 12, 6, 13, 14, 15]. An increasing trend in the\\nnumber of released LLMs and names of a few significant LLMs\\nproposed over the years are shown in Fig 1 and Fig 2, respec-\\ntively.\\nThe early work on LLMs, such as T5 [10] and mT5 [11] em-\\nployed transfer learning until GPT-3 [6] showed LLMs are\\nzero-shot transferable to downstream tasks without fine-tuning.\\nLLMs accurately respond to task queries when prompted with\\ntask descriptions and examples. However, pre-trained LLMs\\nfail to follow user intent and perform worse in zero-shot set-\\ntings than in few-shot.\\nFine-tuning them with task instruc-\\ntions data [16, 17, 18, 19] and aligning with human prefer-\\nences [20, 21] enhances generalization to unseen tasks, im-\\nproving zero-shot performance significantly and reducing mis-\\naligned behavior.\\nIn addition to better generalization and domain adaptation,\\nLLMs appear to have emergent abilities, such as reasoning,\\nplanning, decision-making, in-context learning, answering in\\nzero-shot settings, etc.\\nThese abilities are known to be ac-\\nquired by them due to their gigantic scale even when the pre-\\ntrained LLMs are not trained specifically to possess these at-\\ntributes [22, 23, 24]. Such abilities have led LLMs to be widely\\nadopted in diverse settings, including multi-modal, robotics,\\ntool manipulation, question answering, autonomous agents, etc.\\nVarious improvements have also been suggested in these areas\\neither by task-specific training [25, 26, 27, 28, 29, 30, 31] or\\nbetter prompting [32].\\nThe LLMs abilities to solve diverse tasks with human-level\\nperformance come at the cost of slow training and inference,\\nextensive hardware requirements, and higher running costs.\\nSuch requirements have limited their adoption and opened up\\nopportunities to devise better architectures [15, 33, 34, 35]\\nand training strategies [36, 37, 21, 38, 39, 40, 41].\\nParam-\\neter efficient tuning [38, 41, 40], pruning [42, 43], quantiza-\\ntion [44, 45], knowledge distillation, and context length inter-\\npolation [46, 47, 48, 49] among others are some of the methods\\nwidely studied for efficient LLM utilization.\\nDue to the success of LLMs on a wide variety of tasks, the\\nresearch literature has recently experienced a large influx of\\nLLM-related contributions.\\nResearchers have organized the\\nLLMs literature in surveys [50, 51, 52, 53], and topic-specific\\nsurveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our\\ncontribution focuses on providing a comprehensive yet concise\\noverview of the general direction of LLM research. This arti-\\ncle summarizes architectural and training details of pre-trained\\nLLMs and delves deeper into the details of concepts like fine-\\ntuning, multi-modal LLMs, augmented LLMs, datasets, eval-\\nuation, applications, challenges, and others to provide a self-\\ncontained comprehensive overview. Our key contributions are\\nsummarized as follows.\\n• We present a survey on the developments in LLM research,\\nproviding a concise, comprehensive overview of the direc-\\ntion.\\n• We present extensive summaries of pre-trained models that\\ninclude fine-grained details of architecture and training de-\\ntails.\\n• We summarize major findings of the popular contributions\\nand provide a detailed discussion on the key design and\\ndevelopment aspects of LLMs to help practitioners effec-\\ntively leverage this technology.\\n• In this self-contained article, we cover a range of con-\\ncepts to present the general direction of LLMs compre-\\nhensively, including background, pre-training, fine-tuning,\\n2'), Document(metadata={}, page_content='Figure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1. Pre-Training 2. Fine-Tuning 3. Efficient 4. Inference 5. Evaluation 6. Applications\\n7. Challenges\\nmulti-modal LLMs, augmented LLMs, LLMs-powered\\nagents, datasets, evaluation, etc.\\nWe loosely follow the existing terminology to ensure a stan-\\ndardized outlook of this research direction. For instance, fol-\\nlowing [50], our survey discusses pre-trained LLMs with 10B\\nparameters or more. We refer the readers interested in smaller\\npre-trained models to [51, 52, 53].\\nThe organization of this paper is as follows. Section 2 discusses\\nthe background of LLMs. Section 3 focuses on LLMs overview,\\narchitectures, training pipelines and strategies, fine-tuning, and\\nutilization in different domains. Section 4 highlights the config-\\nuration and parameters that play a crucial role in the function-\\ning of these models. Summary and discussions are presented\\nin section 3.8. The LLM training and evaluation, datasets, and\\nbenchmarks are discussed in section 5, followed by challenges\\nand future directions, and conclusion in sections 7 and 8, re-\\nspectively.\\n3'), Document(metadata={}, page_content='2. Background\\nWe provide the relevant background to understand the fun-\\ndamentals related to LLMs in this section. We briefly discuss\\nnecessary components in LLMs and refer the readers interested\\nin details to the original works.\\n2.1. Tokenization\\nTokenization [59] is an essential pre-processing step in\\nLLM training that parses the text into non-decomposing units\\ncalled tokens. Tokens can be characters, subwords [60], sym-\\nbols [61], or words, depending on the tokenization process.\\nSome of the commonly used tokenization schemes in LLMs\\ninclude wordpiece [62], byte pair encoding (BPE) [61], and un-\\nigramLM [60]. Readers are encouraged to refer to [63] for a\\ndetailed survey.\\n2.2. Encoding Positions\\nThe transformer processes input sequences in parallel and\\nindependently of each other.\\nMoreover, the attention mod-\\nule in the transformer does not capture positional information.\\nAs a result, positional encodings were introduced in trans-\\nformer [64], where a positional embedding vector is added to\\nthe token embedding. Variants of positional embedding include\\nabsolute, relative, or learned positional encodings. Within rel-\\native encoding, Alibi and RoPE are two widely used positional\\nembeddings in LLMs.\\nAlibi [65]: It subtracts a scalar bias from the attention score\\nthat increases with the distance between token positions. This\\nfavors using recent tokens for attention.\\nRoPE [66]: It rotates query and key representations at an an-\\ngle proportional to the token absolute position in the input\\nsequence, resulting in a relative positional encoding scheme\\nwhich decays with the distance between the tokens.\\n2.3. Attention in LLMs\\nAttention assigns weights to input tokens based on impor-\\ntance so that the model gives more emphasis to relevant tokens.\\nAttention in transformers [64] calculates query, key, and value\\nmappings for input sequences, where the attention score is\\nobtained by multiplying the query and key, and later used to\\nweight values. We discuss different attention strategies used in\\nLLMs below.\\nSelf-Attention [64]: Calculates attention using queries, keys,\\nand values from the same block (encoder or decoder).\\nCross Attention: It is used in encoder-decoder architectures,\\nwhere encoder outputs are the queries, and key-value pairs\\ncome from the decoder.\\nSparse Attention [67]: Self-attention has O(n2) time complex-\\nity which becomes infeasible for large sequences. To speed\\nup the computation, sparse attention [67] iteratively calculates\\nattention in sliding windows for speed gains.\\nFlash Attention [68]: Memory access is the major bottleneck\\nin calculating attention using GPUs.\\nTo speed up, flash\\nattention employs input tiling to minimize the memory reads\\nand writes between the GPU high bandwidth memory (HBM)\\nand the on-chip SRAM.\\n2.4. Activation Functions\\nThe activation functions serve a crucial role in the curve-\\nfitting abilities of neural networks [69]. We discuss activation\\nfunctions used in LLMs in this section.\\nReLU [70]: The Rectified linear unit (ReLU) is defined as:\\nReLU(x) = max(0, x)\\n(1)\\nGeLU [71]: The Gaussian Error Linear Unit (GeLU) is the\\ncombination of ReLU, dropout [72] and zoneout [73].\\nGLU variants [74]: The Gated Linear Unit [75] is a neural\\nnetwork layer that is an element-wise product (⊗) of a linear\\ntransformation and a sigmoid transformed (σ) linear projection\\nof the input given as:\\nGLU(x, W, V, b, c) = (xW + b) ⊗σ(xV + c),\\n(2)\\nwhere X is the input of layer and l, W, b, V and c are learned\\nparameters. Other GLU variants [74] used in LLMs are:\\nReGLU(x, W, V, b, c) = max(0, xW + b)⊗,\\nGEGLU(x, W, V, b, c) = GELU(xW + b) ⊗(xV + c),\\nS wiGLU(x, W, V, b, c, β) = S wishβ(xW + b) ⊗(xV + c).\\n2.5. Layer Normalization\\nLayer normalization leads to faster convergence and is an in-\\ntegrated component of transformers [64]. In addition to Layer-\\nNorm [76] and RMSNorm [77], LLMs use pre-layer normal-\\nization [78], applying it before multi-head attention (MHA).\\nPre-norm is shown to provide training stability in LLMs. An-\\nother normalization variant, DeepNorm [79] fixes the issue with\\nlarger gradients in pre-norm.\\n2.6. Distributed LLM Training\\nThis section describes distributed LLM training approaches\\nbriefly. More details are available in [13, 37, 80, 81].\\nData Parallelism: Data parallelism replicates the model on\\nmultiple devices where data in a batch gets divided across de-\\nvices. At the end of each training iteration weights are synchro-\\nnized across all devices.\\nTensor Parallelism: Tensor parallelism shards a tensor compu-\\ntation across devices. It is also known as horizontal parallelism\\nor intra-layer model parallelism.\\nPipeline Parallelism: Pipeline parallelism shards model layers\\nacross different devices. This is also known as vertical paral-\\nlelism.\\nModel Parallelism: A combination of tensor and pipeline par-\\nallelism is known as model parallelism.\\n3D Parallelism: A combination of data, tensor, and model par-\\nallelism is known as 3D parallelism.\\nOptimizer Parallelism: Optimizer parallelism also known as\\nzero redundancy optimizer [37] implements optimizer state\\npartitioning, gradient partitioning, and parameter partitioning\\nacross devices to reduce memory consumption while keeping\\nthe communication costs as low as possible.\\n4'), Document(metadata={}, page_content='2.7. Libraries\\nSome commonly used libraries for LLMs training are:\\nTransformers [82]: The library provides access to various pre-\\ntrained transformer models with APIs to train, fine-tune, infer,\\nand develop custom models.\\nDeepSpeed [36]: A library for scalable distributed training and\\ninference of deep learning models.\\nMegatron-LM [80]: It provides GPU-optimized techniques for\\nlarge-scale training of LLMs.\\nJAX [83]: A Python library for high-performance numerical\\ncomputing and scaleable machine learning. It can differenti-\\nate native Python and NumPy functions and execute them on\\nGPUs.\\nColossal-AI [84]: A collection of components to write dis-\\ntributed deep learning models.\\nBMTrain [81]: A library to write efficient stand-alone LLMs\\ntraining code.\\nFastMoE [85]:\\nProvides API to build mixture-of-experts\\n(MoE) model in PyTorch.\\nMindSpore [86]: A deep learning training and inference frame-\\nwork extendable to mobile, edge, and cloud computing.\\nPyTorch [87]: A framework developed by Facebook AI Re-\\nsearch lab (FAIR) to build deep learning models. The main\\nfeatures of PyTorch include a dynamic computation graph and\\na pythonic coding style.\\nTensorflow [88]:\\nA deep learning framework written by\\nGoogle. The key features of TensorFlow are graph-based com-\\nputation, eager execution, scalability, etc.\\nMXNet [89]: Apache MXNet is a deep learning framework\\nwith support to write programs in multiple languages, includ-\\ning, Python, C++, Scala, R, etc. It also provides support for\\ndynamic and static computation graphs.\\n2.8. Data PreProcessing\\nThis section briefly summarizes data preprocessing tech-\\nniques used in LLMs training.\\nQuality Filtering: For better results, training data quality is\\nessential. Some approaches to filtering data are: 1) classifier-\\nbased and 2) heuristics-based.\\nClassifier-based approaches\\ntrain a classifier on high-quality data and predict the quality of\\ntext for filtering, whereas heuristics-based employ some rules\\nfor filtering like language, metrics, statistics, and keywords.\\nData Deduplication: Duplicated data can affect model per-\\nformance and increase data memorization; therefore, to train\\nLLMs, data deduplication is one of the preprocessing steps.\\nThis can be performed at multiple levels, like sentences,\\ndocuments, and datasets.\\nPrivacy Reduction: Most of the training data for LLMs is\\ncollected through web sources.\\nThis data contains private\\ninformation; therefore, many LLMs employ heuristics-based\\nmethods to filter information such as names, addresses, and\\nphone numbers to avoid learning personal information.\\n2.9. Architectures\\nHere we discuss the variants of the transformer architectures\\nused in LLMs. The difference arises due to the application of\\nFigure 4: An example of attention patterns in language models, image is taken\\nfrom [93].\\nFigure 5: An example of language model training objectives, image from [93].\\nthe attention and the connection of transformer blocks. An il-\\nlustration of attention patterns of these architectures is shown\\nin Figure 4.\\nEncoder Decoder: This architecture processes inputs through\\nthe encoder and passes the intermediate representation to the\\ndecoder to generate the output.\\nHere, the encoder sees the\\ncomplete sequence utilizing self-attention whereas the decoder\\nprocesses the sequence one after the other with implementing\\ncross-attention.\\nCausal Decoder: A type of architecture that does not have an\\nencoder and processes and generates output using a decoder,\\nwhere the predicted token depends only on the previous time\\nsteps.\\nPrefix Decoder: It is also known as a non-causal decoder,\\nwhere the attention calculation is not strictly dependent on the\\npast information and the attention is bidirectional. An example\\nof a non-causal attention mask is shown in Figure 4.\\nMixture-of-Experts: It is a variant of transformer architecture\\nwith parallel independent experts and a router to route tokens\\nto experts. These experts are feed-forward layers after the at-\\ntention block [90]. Mixture-of-Experts (MoE) is an efficient\\nsparse architecture that offers comparable performance to dense\\nmodels and allows increasing the model size without increas-\\ning the computational cost by activating only a few experts at a\\ntime [91, 92].\\n2.10. Pre-Training Objectives\\nThis section describes LLMs pre-training objectives.\\nFor\\nmore details see the paper [93].\\nFull Language Modeling: An autoregressive language model-\\ning objective where the model is asked to predict future tokens\\ngiven the previous tokens, an example is shown in Figure 5.\\nPrefix Language Modeling: A non-causal training objective,\\nwhere a prefix is chosen randomly and only remaining target\\ntokens are used to calculate the loss. An example is shown in\\nFigure 5.\\n5'), Document(metadata={}, page_content='Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting/utilization. Prompting LLMs to generate responses is possible at\\ndifferent training stages like pre-training, instruction-tuning, or alignment tuning. “RL” stands for reinforcement learning, “RM” represents reward-modeling, and\\n“RLHF” represents reinforcement learning with human feedback.\\nMasked Language Modeling: In this training objective, tokens\\nor spans (a sequence of tokens) are masked randomly and the\\nmodel is asked to predict masked tokens given the past and\\nfuture context. An example is shown in Figure 5.\\nUnified Language Modeling: Unified language modeling [94]\\nis a combination of causal, non-causal, and masked language\\ntraining objectives. Here in masked language modeling, the\\nattention is not bidirectional but unidirectional, attending either\\nleft-to-right or right-to-left context.\\n2.11. LLMs Scaling Laws\\nScaling laws study the optimal combination of model param-\\neters, dataset size, and computational resources that predict the\\nimprovement in the model performance. It has been shown\\nthat the loss scales according to the power-law with model size,\\ndataset size, and compute resources [95]. This study suggests\\nlarger models are more important than big data for better perfor-\\nmance. Another variant of scaling law [96] suggests the model\\nsize and the number of training tokens should be scaled equally.\\n2.12. LLMs Adaptation Stages\\nThis section discusses the fundamentals of LLMs adaptation\\nstages, from pre-training to fine-tuning for downstream tasks\\nand utilization. An example of different training stages and in-\\nference in LLMs is shown in Figure 6. In this paper, we refer\\nto alignment-tuning as aligning with human preferences, while\\noccasionally the literature uses the term alignment for different\\npurposes.\\n2.12.1. Pre-Training\\nIn the very first stage, the model is trained in a self-\\nsupervised manner on a large corpus to predict the next to-\\nkens given the input. The design choices of LLMs vary from\\nencoder-decoder to decoder-only architectures with different\\nbuilding blocks and loss functions in sections 2.5, 2.4, 2.10.\\n2.12.2. Fine-Tuning\\nThere are different styles to fine-tune an LLM. This section\\nbriefly discusses fine-tuning approaches.\\nTransfer Learning: The pre-trained LLMs perform well for\\nvarious tasks [6, 15]. However, to improve the performance for\\n6'), Document(metadata={}, page_content='a downstream task, pre-trained models are fine-tuned with the\\ntask-specific data [10, 11], known as transfer learning.\\nInstruction-tuning: To enable a model to respond to user\\nqueries effectively, the pre-trained model is fine-tuned on in-\\nstruction formatted data i.e., instruction and an input-output\\npair. Instructions generally comprise multi-task data in plain\\nnatural language, guiding the model to respond according to the\\nprompt and the input. This type of fine-tuning improves zero-\\nshot generalization and downstream task performance. Details\\non formatting instruction data and its various styles are avail-\\nable in [16, 50, 97].\\nAlignment-tuning: LLMs are prone to generating false, biased,\\nand harmful text. To make them helpful, honest, and harmless,\\nmodels are aligned using human feedback. Alignment involves\\nasking LLMs to generate unexpected responses and then updat-\\ning their parameters to avoid such responses [20, 21, 98].\\nIt ensures LLMs operate according to human intentions and\\nvalues. A model is defined to be an “aligned” model if the\\nmodel fulfills three criteria of helpful, honest, and harmless or\\n“HHH” [99].\\nResearchers employ reinforcement learning with human feed-\\nback (RLHF) [100] for model alignment. In RLHF, a fine-tuned\\nmodel on demonstrations is further trained with reward model-\\ning (RM) and reinforcement learning (RL), shown in Figure 6.\\nBelow we briefly discuss RM and RL pipelines in RLHF.\\nReward modeling: trains a model to rank generated responses\\naccording to human preferences using a classification objec-\\ntive. To train the classifier humans annotate LLMs generated\\nresponses based on the HHH criteria.\\nReinforcement learning: in combination with the reward model\\nis used for alignment in the next stage. The previously trained\\nreward model ranks LLM-generated responses into preferred\\nvs. non-preferred, which is used to align the model with proxi-\\nmal policy optimization (PPO). This process repeats iteratively\\nuntil convergence.\\n2.12.3. Prompting/Utilization\\nPrompting is a method to query trained LLMs for generating\\nresponses, as illustrated in Figure 6. LLMs can be prompted in\\nvarious prompt setups, where they can be adapted to the instruc-\\ntions without fine-tuning and in other cases with fine-tuning on\\ndata containing different prompt styles [16, 101, 102]. A good\\nguide on prompt engineering is available at [32]. Below, we\\nwill discuss various widely used prompt setups.\\nZero-Shot Prompting: LLMs are zero-shot learners and ca-\\npable of answering queries never seen before. This style of\\nprompting requires LLMs to answer user questions without see-\\ning any examples in the prompt.\\nIn-context Learning: Also known as few-shot learning, here,\\nmultiple input-output demonstration pairs are shown to the\\nmodel to generate the desired response. This adaptation style\\nis also called few-shot learning. A discussion on formatting in-\\ncontext learning (ICL) templates is available in [54, 50, 18, 16].\\nReasoning in LLMs: LLMs are zero-shot reasoners and can\\nbe provoked to generate answers to logical problems, task\\nplanning, critical thinking, etc. with reasoning.\\nGenerating\\nreasons is possible only by using different prompting styles,\\nwhereas to improve LLMs further on reasoning tasks many\\nmethods [16, 97] train them on reasoning datasets. We discuss\\nvarious prompting techniques for reasoning below.\\nChain-of-Thought (CoT): A special case of prompting where\\ndemonstrations contain reasoning information aggregated with\\ninputs and outputs so that the model generates outcomes with\\nstep-by-step reasoning. More details on CoT prompts are avail-\\nable in [55, 103, 101].\\nSelf-Consistency:\\nImproves CoT performance by generat-\\ning multiple responses and selecting the most frequent an-\\nswer [104].\\nTree-of-Thought (ToT): Explores multiple reasoning paths\\nwith possibilities to look ahead and backtrack for problem-\\nsolving [105].\\nSingle-Turn Instructions: In this prompting setup, LLMs are\\nqueried only once with all the relevant information in the\\nprompt. LLMs generate responses by understanding the con-\\ntext either in a zero-shot or few-shot setting.\\nMulti-Turn Instructions: Solving a complex task requires mul-\\ntiple interactions with LLMs, where feedback and responses\\nfrom the other tools are given as input to the LLM for the next\\nrounds. This style of using LLMs in the loop is common in\\nautonomous agents.\\n3. Large Language Models\\nThis section reviews LLMs, briefly describing their architec-\\ntures, training objectives, pipelines, datasets, and fine-tuning\\ndetails.\\n3.1. Pre-Trained LLMs\\nHere, we provide summaries of various well-known pre-\\ntrained LLMs with significant discoveries, changing the course\\nof research and development in NLP. These LLMs have consid-\\nerably improved the performance in NLU and NLG domains,\\nand are widely fine-tuned for downstream tasks. Moreover, We\\nalso identify key findings and insights of pre-trained LLMs in\\nTable 1 and 2 that improve their performance.\\n3.1.1. General Purpose\\nT5 [10]: An encoder-decoder model employing a unified text-\\nto-text training for all NLP problems is shown in Figure 7. T5\\nplaces layer normalization outside the residual path in a conven-\\ntional transformer model [64]. It uses masked language mod-\\neling as a pre-training objective where spans (consecutive to-\\nkens) are replaced with a single mask instead of separate masks\\nfor each token. This type of masking speeds up the training as\\nit produces shorter sequences. After pre-training, the model is\\nfine-tuned using adapter layers [106] for downstream tasks.\\nGPT-3 [6]: The GPT-3 architecture is the same as the GPT-\\n2 [5] but with dense and sparse attention in transformer layers\\nsimilar to the Sparse Transformer [67]. It shows that large mod-\\nels can train on larger batch sizes with a lower learning rate to\\ndecide the batch size during training, GPT-3 uses the gradient\\nnoise scale as in [107]. Overall, GPT-3 increases model param-\\neters to 175B showing that the performance of large language\\n7'), Document(metadata={}, page_content='Figure 7: Unified text-to-text training example, source image from [10].\\nFigure 8: The image is the article of [108], showing an example of PanGu-α\\narchitecture.\\nmodels improves with the scale and is competitive with the fine-\\ntuned models.\\nmT5 [11]: A multilingual T5 model [10] trained on the mC4\\ndataset with 101 languages. The dataset is extracted from the\\npublic common crawl scrape. The model uses a larger vocab-\\nulary size of 250,000 to cover multiple languages. To avoid\\nover-fitting or under-fitting for a language, mT5 employs a data\\nsampling procedure to select samples from all languages. The\\npaper suggests using a small amount of pre-training datasets,\\nincluding all languages when fine-tuning for a task using En-\\nglish language data. This allows the model to generate correct\\nnon-English outputs.\\nPanGu-α [108]: An autoregressive model that has a query\\nlayer at the end of standard transformer layers, example shown\\nin Figure 8, to predict the next token. Its structure is similar to\\nthe transformer layer but with an additional embedding for the\\nnext position in the attention mechanism, given in Eq. 3.\\na = pnWq\\nhWk\\nhTHT\\nL\\n(3)\\nCPM-2 [12]: Cost-efficient Pre-trained language Models\\n(CPM-2) pre-trains bilingual (English and Chinese) 11B and\\n198B mixture-of-experts (MoE) models on the WuDaoCor-\\npus [109] dataset. The tokenization process removes “_” white\\nspace tokens in the sentencepiece tokenizer. The models are\\ntrained with knowledge inheritance, starting with only the Chi-\\nnese language in the first stage and then adding English and\\nChinese data. This trained model gets duplicated multiple times\\nto initialize the 198B MoE model. Moreover, to use the model\\nfor downstream tasks, CPM-2 experimented with both com-\\nplete fine-tuning and prompt fine-tuning as in [40] where only\\nprompt-related parameters are updated by inserting prompts at\\nvarious positions, front, middle, and back. CPM-2 also pro-\\nposes the INFMOE, a memory-efficient framework with a strat-\\negy to dynamically offload parameters to the CPU for inference\\nat a 100B scale. It overlaps data movement with inference com-\\nputation for lower inference time.\\nERNIE 3.0 [110]: ERNIE 3.0 takes inspiration from multi-\\ntask learning to build a modular architecture using Transformer-\\nXL [111] as the backbone. The universal representation mod-\\nule is shared by all the tasks, which serve as the basic block\\nfor task-specific representation modules, which are all trained\\njointly for natural language understanding, natural language\\ngeneration, and knowledge extraction. This LLM is primar-\\nily focused on the Chinese language. It claims to train on the\\nlargest Chinese text corpora for LLM training, and achieved\\nstate-of-the-art in 54 Chinese NLP tasks.\\nJurassic-1 [112]: A pair of auto-regressive language mod-\\nels, including a 7B-parameter J1-Large model and a 178B-\\nparameter J1-Jumbo model.\\nThe training vocabulary of\\nJurassic-1 comprise word pieces, complete words, and multi-\\nword expressions without any word boundaries, where possible\\nout-of-vocabulary instances are interpreted as Unicode bytes.\\nCompared to the GPT-3 counterparts, the Jurassic-1 models\\napply a more balanced depth-to-width self-attention architec-\\nture [113] and an improved tokenizer for a faster prediction\\nbased on broader resources, achieving a comparable perfor-\\nmance in zero-shot learning tasks and a superior performance in\\nfew-shot learning tasks given the ability to feed more examples\\nas a prompt.\\nHyperCLOVA [114]: A Korean language model with GPT-3\\narchitecture.\\nYuan 1.0 [115]: Trained on a Chinese corpus with 5TB of\\nhigh-quality text collected from the Internet. A Massive Data\\nFiltering System (MDFS) built on Spark is developed to pro-\\ncess the raw data via coarse and fine filtering techniques. To\\nspeed up the training of Yuan 1.0 to save energy expenses and\\ncarbon emissions, various factors that improve the performance\\nof distributed training are incorporated in architecture and train-\\ning: like increasing the hidden state size improves pipeline and\\ntensor parallelism performance, larger micro batches improve\\npipeline parallelism performance, and larger global batch size\\nimprove data parallelism performance. In practice, the Yuan 1.0\\nmodel performs well on text classification, Winograd Schema,\\nnatural language inference, and reading comprehension tasks.\\nGopher [116]: The Gopher family of models ranges from\\n44M to 280B parameters in size to study the effect of scale\\non the LLMs performance. The 280B model beats GPT-3 [6],\\nJurrasic-1 [112], MT-NLG [117], and others on 81% of the\\nevaluated tasks.\\nERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan extends ERNIE 3.0\\nby training a larger model with 26x the number of parameters\\nof the latter. This bigger model outperformed other state-of-the-\\nart models in 68 NLP tasks. LLMs produce text with incorrect\\nfacts. In order to have control of the generated text with fac-\\ntual consistency, ERNIE 3.0 Titan adds another task, Credible\\nand Controllable Generations, to its multi-task learning setup.\\n8'), Document(metadata={}, page_content='It introduces additional self-supervised adversarial and control-\\nlable language modeling losses to the pre-training step, which\\nenables ERNIE 3.0 Titan to beat other LLMs in their manually\\nselected Factual QA task set evaluations.\\nGPT-NeoX-20B [118]: An auto-regressive model that largely\\nfollows GPT-3 with a few deviations in architecture design,\\ntrained on the Pile dataset without any data deduplication. GPT-\\nNeoX has parallel attention and feed-forward layers in a trans-\\nformer block, given in Eq. 4, that increases throughput by 15%.\\nIt uses rotary positional embedding [66], applying it to only\\n25% of embedding vector dimension as in [119]. This reduces\\nthe computation without performance degradation. As opposed\\nto GPT-3, which uses dense and sparse layers, GPT-NeoX-20B\\nuses only dense layers. The hyperparameter tuning at this scale\\nis difficult; therefore, the model chooses hyperparameters from\\nthe method [6] and interpolates values between 13B and 175B\\nmodels for the 20B model. The model training is distributed\\namong GPUs using both tensor and pipeline parallelism.\\nx + Attn(LN1(x)) + FF(LN2(x))\\n(4)\\nOPT [14]: It is a clone of GPT-3, developed to open-source\\na model that replicates GPT-3 performance. Training of OPT\\nemploys dynamic loss scaling [120] and restarts from an earlier\\ncheckpoint with a lower learning rate whenever loss divergence\\nis observed. Overall, the performance of OPT-175B models is\\ncomparable to the GPT3-175B model.\\nBLOOM [13]: A causal decoder model trained on the ROOTS\\ncorpus to open-source an LLM. The architecture of BLOOM is\\nshown in Figure 9, with differences like ALiBi positional em-\\nbedding, an additional normalization layer after the embedding\\nlayer as suggested by the bitsandbytes1 library. These changes\\nstabilize training with improved downstream performance.\\nGLaM [91]: Generalist Language Model (GLaM) represents a\\nfamily of language models using a sparsely activated decoder-\\nonly mixture-of-experts (MoE) structure [121, 90].\\nTo gain\\nmore model capacity while reducing computation, the experts\\nare sparsely activated where only the best two experts are used\\nto process each input token. The largest GLaM model, GLaM\\n(64B/64E), is about 7× larger than GPT-3 [6], while only part of\\nthe parameters are activated per input token. The largest GLaM\\n(64B/64E) model achieves better overall results as compared\\nto GPT-3 while consuming only one-third of GPT-3’s training\\nenergy.\\nMT-NLG [117]: A 530B causal decoder based on the GPT-\\n2 architecture that has roughly 3× GPT-3 model parameters.\\nMT-NLG is trained on filtered high-quality data collected from\\nvarious public datasets and blends various types of datasets in a\\nsingle batch, which beats GPT-3 on several evaluations.\\nChinchilla [96]: A causal decoder trained on the same dataset\\nas the Gopher [116] but with a little different data sampling\\ndistribution (sampled from MassiveText). The model architec-\\nture is similar to the one used for Gopher, with the exception of\\nAdamW optimizer instead of Adam. Chinchilla identifies the\\n1https://github.com/TimDettmers/bitsandbytes\\nFigure 9: The BLOOM architecture example sourced from [13].\\nrelationship that model size should be doubled for every dou-\\nbling of training tokens. Over 400 language models ranging\\nfrom 70 million to over 16 billion parameters on 5 to 500 bil-\\nlion tokens are trained to get the estimates for compute-optimal\\ntraining under a given budget. The authors train a 70B model\\nwith the same compute budget as Gopher (280B) but with 4\\ntimes more data. It outperforms Gopher [116], GPT-3 [6], and\\nothers on various downstream tasks, after fine-tuning.\\nAlexaTM [122]: An encoder-decoder model, where encoder\\nweights and decoder embeddings are initialized with a pre-\\ntrained encoder to speed up training. The encoder stays frozen\\nfor the initial 100k steps and is later unfrozen for end-to-end\\ntraining. The model is trained on a combination of denoising\\nand causal language modeling (CLM) objectives, concatenat-\\ning a [CLM] token at the beginning for mode switching. Dur-\\ning training, the CLM task is applied for 20% of the time, which\\nimproves the in-context learning performance.\\nPaLM [15]: A causal decoder with parallel attention and\\nfeed-forward layers similar to Eq. 4, speeding up training by\\na factor of 15. Additional changes to the conventional trans-\\nformer model include SwiGLU activation, RoPE embeddings,\\nmulti-query attention that saves computation cost during decod-\\ning, and shared input-output embeddings. During training, loss\\nspiking was observed, and to fix it, model training was restarted\\nfrom a 100-step earlier checkpoint by skipping 200-500 batches\\naround the spike. Moreover, the model was found to memo-\\nrize around 2.4% of the training data at the 540B model scale,\\nwhereas this number was lower for smaller models.\\nPaLM-2 [123]: A smaller multi-lingual variant of PaLM,\\ntrained for larger iterations on a better quality dataset. PaLM-\\n2 shows significant improvements over PaLM, while reducing\\ntraining and inference costs due to its smaller size. To lessen\\ntoxicity and memorization, it appends special tokens with a\\nfraction of pre-training data, which shows a reduction in gener-\\nating harmful responses.\\nU-PaLM [124]: This method trains PaLM for 0.1% addi-\\ntional compute with the UL2 (also named as UL2Restore) ob-\\njective [125], using the same dataset it outperforms the baseline\\nsignificantly on various NLP tasks, including zero-shot, few-\\nshot, commonsense reasoning, CoT, etc. Training with UL2R\\ninvolves converting a causal decoder PaLM to a non-causal de-\\ncoder PaLM and employing 50% sequential denoising, 25%\\nregular denoising, and 25% extreme denoising loss functions.\\n9'), Document(metadata={}, page_content='UL2 [125]: An encoder-decoder architecture trained using a\\nmixture of denoisers (MoD) objective. Denoisers include 1)\\nR-Denoiser: a regular span masking, 2) S-Denoiser: which cor-\\nrupts consecutive tokens of a large sequence and 3) X-Denoiser:\\nwhich corrupts a large number of tokens randomly. During pre-\\ntraining, UL2 includes a denoiser token from R, S, X to rep-\\nresent a denoising setup. It helps improve fine-tuning perfor-\\nmance for downstream tasks that bind the task to one of the up-\\nstream training modes. This MoD style of training outperforms\\nthe T5 model on many benchmarks.\\nGLM-130B [33]: GLM-130B is a bilingual (English and Chi-\\nnese) model trained using an auto-regressive mask infilling pre-\\ntraining objective similar to the GLM [126]. This training style\\nmakes the model bidirectional as compared to GPT-3, which is\\nunidirectional. As opposed to GLM, the training of GLM-130B\\nincludes a small amount of multi-task instruction pre-training\\ndata (5% of the total data) along with self-supervised mask in-\\nfilling. To stabilize the training, it applies embedding layer gra-\\ndient shrink.\\nLLaMA [127, 21]: A set of decoder-only language models\\nvarying from 7B to 70B parameters. LLaMA models series is\\nthe most famous among the community for parameter efficiency\\nand instruction tuning.\\nLLaMA-1 [127]: Implements efficient causal attention [128]\\nby not storing and computing masked attention weights and\\nkey/query scores. Another optimization is reducing the number\\nof activations recomputed in the backward pass, as in [129].\\nLLaMA-2 [21]: This work is more focused on fine-tuning a\\nsafer and better LLaMA-2-Chat model for dialogue generation.\\nThe pre-trained model has 40% more training data with a larger\\ncontext length and grouped-query attention.\\nLLaMA-3/3.1 [130]: A collection of models trained on a\\nseven times larger dataset as compared to LLaMA-2 with dou-\\nble the context length, outperforming its previous variants and\\nother models.\\nPanGu-Σ [92]: An autoregressive model with parameters\\ncopied from PanGu-α and extended to a trillion scale with Ran-\\ndom Routed Experts (RRE), the architectural diagram is shown\\nin Figure 10. RRE is similar to the MoE architecture, with\\ndistinctions at the second level, where tokens are randomly\\nrouted to experts in a domain instead of using a learnable gat-\\ning method. The model has bottom layers densely activated and\\nshared across all domains, whereas top layers are sparsely ac-\\ntivated according to the domain. This training style allows for\\nextracting task-specific models and reduces catastrophic forget-\\nting effects in the case of continual learning.\\nMixtral8x22b [131]: A mixture-of-experts (MoE) model with\\neight distinct experts routes each token to two experts at each\\nlayer and combines the outputs additively.\\nSnowflake Arctic [132]: Arctic LLM is a hybrid of dense and\\nmixture-of-experts (MoE) architecture. The MoE (128×3.66B\\nMLP experts) is parallel to the dense transformer (10B) with\\nonly two experts activated. The model has many experts, com-\\npared to other MoE LLMs [131, 133], to increase the model\\ncapacity and provide an opportunity to choose among many ex-\\nperts for a diverse configuration. The model has 480B param-\\neters, and only 17B are active during a forward pass, reducing\\nthe computation significantly.\\nGrok [133, 134]: Grok is a family of LLMs including Grok-1\\nand Grok-1.5, released by XAI.\\nGrok-1 [133]: Grok-1 is a 314B parameters language MoE\\nmodel (eight experts), where two experts are activated per to-\\nken.\\nGrok-1.5 [134]: Grok-1.5 is a multi-modal LLM with a larger\\ncontext length and improved performance.\\nGemini [135, 136]: Gemini replaces Bard (based on PaLM)\\nwith multi-modal capabilities and significant language model-\\ning performance improvements.\\nGemini-1 [135]: The first-ever auto-regressive model to\\nachieve human-level capabilities on the MMLU benchmark.\\nGemini-1.5 [136]: A multi-modal LLM with MoE architec-\\nture builds on the findings of Gemini-1. The model has a 2M\\ncontext window and can reason over information up to 10M\\ntokens. Such large context windows were never achieved pre-\\nviously and shown to have a huge impact on performance gain.\\nNemotron-4 340B [137]: A decoder-only model that has been\\naligned on 98% synthetic data and only 2% manually annotated\\ndata. Utilizing synthetic data at a large proportion improves the\\nmodel performance significantly. The paper suggested intro-\\nducing alignment data with a smaller subset of previously seen\\ndata during the late stage of the model pre-training, enabling the\\nsmooth transition from the pre-trained stage to the final train-\\ning stage. To train better instruction-following models, weaker\\nmodels are trained into stronger models iteratively. The syn-\\nthetic data generated by the weaker instruction-tuned model is\\nused to train a base model which is later supervised fine-tuned\\noutperforming the weaker model.\\nDeepSeek [138]: DeepSeek studies the LLMs scaling laws\\nin detail to determine the optimal non-embedding model size\\nand training data. The experiments were performed for 8 bud-\\ngets ranging from 1e17 to 3e20 training FLOPs. Each compute\\nbudget was tested against ten different models/data scales. The\\nbatch size and learning rates were also fitted for the given com-\\npute budget finding that the batch size should increase with\\nthe increased compute budget while decreasing the learning\\nrate. Following are the equations for the optimal batch-size (B),\\nlearning rate (η), model size (M), and data (D):\\nBopt = 0.2920.C0.3271\\nηopt = 0.3118.C−0.1250\\nMopt = Mbase.Ca\\nDopt = Dbase.Cb\\nMbase = 0.1715, Dbase = 5.8316, a = 0.5243, b = 0.4757\\n(5)\\nDeepSeek-v2 [139]: An MoE model that introduces multi-\\nhead latent attention (MLA) to reduce inference costs, by com-\\npressing Key-Value (KV) cache into a latent vector.\\nMLA\\nachieves better performance than multi-head attention (MHA),\\nand other efficient attention mechanisms such as grouped query\\nattention (GQA), multi-query attention (MQA), etc. Because\\nof MLA, DeepSeek-v2 achieves 5.76 times faster inference\\nthroughput as compared to DeepSeek [138].\\n10')]}}\n",
      "{'build_vector_store_retrieval': {'retrieval_content': [\"page_content='A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,\\nNick Barnesi, Ajmal Mianj\\naThe University of Sydney, Sydney, Australia\\nbUniversity of Engineering and Technology (UET), Lahore, Pakistan\\ncThe Chinese University of Hong Kong (CUHK), HKSAR, China\\ndUniversity of Technology Sydney (UTS), Sydney, Australia\\neCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydne'\", \"page_content='A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,\\nNick Barnesi, Ajmal Mianj\\naThe University of Sydney, Sydney, Australia\\nbUniversity of Engineering and Technology (UET), Lahore, Pakistan\\ncThe Chinese University of Hong Kong (CUHK), HKSAR, China\\ndUniversity of Technology Sydney (UTS), Sydney, Australia\\neCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydne'\", \"page_content='A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,\\nNick Barnesi, Ajmal Mianj\\naThe University of Sydney, Sydney, Australia\\nbUniversity of Engineering and Technology (UET), Lahore, Pakistan\\ncThe Chinese University of Hong Kong (CUHK), HKSAR, China\\ndUniversity of Technology Sydney (UTS), Sydney, Australia\\neCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydne'\"]}}\n",
      "{'summary': {'query_summary': AIMessage(content='1. A Large Language Model (LLM) is a type of artificial intelligence technology that is designed to understand and generate human-like text. Its architecture typically involves a transformer-based model with a large number of parameters. The training process for an LLM involves learning from a vast amount of text data, often using techniques like self-supervised learning. LLMs can be applied in various areas, such as machine translation, text summarization, and question-answering systems.\\n\\n2. The key components of an LLM include data processing, model size, and the generation of human-like text. Data processing involves pre-training the model on a massive corpus of text data, which allows the model to learn the statistical patterns and structures of language. The model size of an LLM is typically very large, often containing billions of parameters. This large size enables the model to generate high-quality, human-like text.\\n\\n3. LLMs are designed to understand and generate text in a way that is similar to how humans communicate. They operate by using complex algorithms and statistical models to analyze and predict the patterns and structures of language. LLMs have various use cases, such as virtual assistants, chatbots, and content generation tools. However, they also have limitations, such as a lack of common sense and the potential for generating biased or inappropriate text. Despite these limitations, LLMs have significant future potential, as they continue to improve and become more sophisticated, they could revolutionize the way we interact with computers and artificial intelligence.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 323, 'prompt_tokens': 697, 'total_tokens': 1020, 'completion_time': 0.501640527, 'prompt_time': 0.034242376, 'queue_time': 0.12826423, 'total_time': 0.535882903}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-4f16948e-4373-40d3-add3-f6446127bb74-0', usage_metadata={'input_tokens': 697, 'output_tokens': 323, 'total_tokens': 1020})}}\n"
     ]
    }
   ],
   "source": [
    "query = {'query':'What is a Large Language Model (LLM), and how does it work?',}\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "\n",
    "for event in compiled_builder.stream(query,thread):\n",
    "    \n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# TAVILY_API_KEY= os.getenv('TAVILY_API_KEY')\n",
    "\n",
    "\n",
    "# tavily_search = TavilySearchResults(api_key=TAVILY_API_KEY)\n",
    "\n",
    "# search_results = tavily_search.invoke(\"latest AI trends\")\n",
    "\n",
    "# contents = [result['content'] for result in search_results]\n",
    "# print(contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import PyMuPDFLoader\n",
    "# from langchain.schema import Document\n",
    "\n",
    "# # Load the PDF file\n",
    "# pdf_loader = PyMuPDFLoader(\"./how_llms_work.pdf\")\n",
    "# pdf_list = []\n",
    "# # Extract documents (text)\n",
    "# documents = pdf_loader.load()\n",
    "# print(documents)\n",
    "# # Print extracted text\n",
    "# for doc in documents:\n",
    "#     pdf_list.append(doc.page_content)\n",
    "\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     separators=[\"\\n\\n\", \"\\n\"],\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=100\n",
    "# )\n",
    "\n",
    "# splits = splitter.split_documents(documents)\n",
    "\n",
    "# # # Print each chunk's content\n",
    "# # for idx, doc in enumerate(splits):\n",
    "# #     print(f\"Chunk {idx + 1}:\")\n",
    "# #     print(doc.page_content)  # This will print the text of each chunk\n",
    "# #     print(\"-\" * 50)  # Separator between chunks\n",
    "\n",
    "# documents12 = [Document(page_content=text) for text in pdf_list]\n",
    "# print(documents12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
